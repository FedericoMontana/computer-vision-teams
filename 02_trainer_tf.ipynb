{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data paths\n",
    "dataset = 'out/gestures.csv'\n",
    "model_save_path = 'models/classifier.hdf5'\n",
    "tflite_save_path = 'models/lite.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random         1874\n",
       "hand_closed     673\n",
       "two_up          648\n",
       "fuck you        634\n",
       "hang_in         623\n",
       "hand_open       581\n",
       "victory         530\n",
       "call            525\n",
       "one_up          416\n",
       "machedici       414\n",
       "ok              409\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(dataset, header=None) \n",
    "\n",
    "y = data.iloc[:, 0]\n",
    "X = data.iloc[: , 1:]\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 0,\n",
       " 'victory': 1,\n",
       " 'call': 2,\n",
       " 'hang_in': 3,\n",
       " 'one_up': 4,\n",
       " 'two_up': 5,\n",
       " 'hand_closed': 6,\n",
       " 'hand_open': 7,\n",
       " 'machedici': 8,\n",
       " 'random': 9,\n",
       " 'fuck you': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = y.unique()\n",
    "classes = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "y = y.map(classes)\n",
    "\n",
    "X = X.to_numpy().astype(np.float32)\n",
    "y = y.to_numpy().astype(np.float32)\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=RANDOM_SEED)\n",
    "\n",
    "# # y_train = y_train.reshape(len(y_train),1)\n",
    "# # y_test = y_test.reshape(len(y_test),1)\n",
    "# classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 17:02:39.385293: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-30 17:02:39.386152: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "n_input = X_train.shape[1]\n",
    "n_output = len(np.unique(y))\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer((n_input, )),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_output, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout (Dropout)           (None, 42)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1376      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 11)                187       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,147\n",
      "Trainable params: 3,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False, save_best_only=True)\n",
    "# Callback for early stopping\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=50, verbose=1)\n",
    "\n",
    "# Model compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 17:02:44.766280: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-07-30 17:02:45.133143: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - ETA: 0s - loss: 2.3005 - accuracy: 0.2479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 17:02:48.133655: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.26033, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 4s 17ms/step - loss: 2.3005 - accuracy: 0.2479 - val_loss: 2.2603 - val_accuracy: 0.2549\n",
      "Epoch 2/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2437 - accuracy: 0.2561\n",
      "Epoch 2: val_loss improved from 2.26033 to 2.19237, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 2.2437 - accuracy: 0.2561 - val_loss: 2.1924 - val_accuracy: 0.2549\n",
      "Epoch 3/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 2.1494 - accuracy: 0.2624\n",
      "Epoch 3: val_loss improved from 2.19237 to 1.99941, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 2.1472 - accuracy: 0.2613 - val_loss: 1.9994 - val_accuracy: 0.2746\n",
      "Epoch 4/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9835 - accuracy: 0.3164\n",
      "Epoch 4: val_loss improved from 1.99941 to 1.77769, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.9838 - accuracy: 0.3163 - val_loss: 1.7777 - val_accuracy: 0.3903\n",
      "Epoch 5/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 1.8115 - accuracy: 0.3733\n",
      "Epoch 5: val_loss improved from 1.77769 to 1.60676, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.8102 - accuracy: 0.3727 - val_loss: 1.6068 - val_accuracy: 0.4689\n",
      "Epoch 6/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6838 - accuracy: 0.4033\n",
      "Epoch 6: val_loss improved from 1.60676 to 1.46684, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.6838 - accuracy: 0.4033 - val_loss: 1.4668 - val_accuracy: 0.4896\n",
      "Epoch 7/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5976 - accuracy: 0.4295\n",
      "Epoch 7: val_loss improved from 1.46684 to 1.39042, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.5958 - accuracy: 0.4302 - val_loss: 1.3904 - val_accuracy: 0.5802\n",
      "Epoch 8/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.4433\n",
      "Epoch 8: val_loss improved from 1.39042 to 1.33802, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.5329 - accuracy: 0.4440 - val_loss: 1.3380 - val_accuracy: 0.5508\n",
      "Epoch 9/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.4824 - accuracy: 0.4537\n",
      "Epoch 9: val_loss improved from 1.33802 to 1.31698, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.4824 - accuracy: 0.4537 - val_loss: 1.3170 - val_accuracy: 0.5786\n",
      "Epoch 10/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4256 - accuracy: 0.4749\n",
      "Epoch 10: val_loss did not improve from 1.31698\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.4251 - accuracy: 0.4750 - val_loss: 1.3852 - val_accuracy: 0.5311\n",
      "Epoch 11/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.3833 - accuracy: 0.4928\n",
      "Epoch 11: val_loss improved from 1.31698 to 1.31622, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.3815 - accuracy: 0.4944 - val_loss: 1.3162 - val_accuracy: 0.5715\n",
      "Epoch 12/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 1.3645 - accuracy: 0.4937\n",
      "Epoch 12: val_loss improved from 1.31622 to 1.21878, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.3633 - accuracy: 0.4935 - val_loss: 1.2188 - val_accuracy: 0.5993\n",
      "Epoch 13/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.3241 - accuracy: 0.5082\n",
      "Epoch 13: val_loss improved from 1.21878 to 1.18815, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.3248 - accuracy: 0.5070 - val_loss: 1.1882 - val_accuracy: 0.5928\n",
      "Epoch 14/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.3033 - accuracy: 0.5089\n",
      "Epoch 14: val_loss improved from 1.18815 to 1.17351, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.3023 - accuracy: 0.5086 - val_loss: 1.1735 - val_accuracy: 0.5873\n",
      "Epoch 15/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.2762 - accuracy: 0.5203\n",
      "Epoch 15: val_loss improved from 1.17351 to 1.16444, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.2742 - accuracy: 0.5194 - val_loss: 1.1644 - val_accuracy: 0.5906\n",
      "Epoch 16/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.2393 - accuracy: 0.5275\n",
      "Epoch 16: val_loss improved from 1.16444 to 1.15293, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.2359 - accuracy: 0.5281 - val_loss: 1.1529 - val_accuracy: 0.5873\n",
      "Epoch 17/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 1.2125 - accuracy: 0.5509\n",
      "Epoch 17: val_loss improved from 1.15293 to 1.15080, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.2186 - accuracy: 0.5496 - val_loss: 1.1508 - val_accuracy: 0.5890\n",
      "Epoch 18/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.1959 - accuracy: 0.5578\n",
      "Epoch 18: val_loss improved from 1.15080 to 1.11613, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1951 - accuracy: 0.5578 - val_loss: 1.1161 - val_accuracy: 0.6190\n",
      "Epoch 19/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.1770 - accuracy: 0.5584\n",
      "Epoch 19: val_loss did not improve from 1.11613\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1748 - accuracy: 0.5585 - val_loss: 1.1650 - val_accuracy: 0.5710\n",
      "Epoch 20/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.1525 - accuracy: 0.5706\n",
      "Epoch 20: val_loss improved from 1.11613 to 1.08598, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1529 - accuracy: 0.5703 - val_loss: 1.0860 - val_accuracy: 0.6103\n",
      "Epoch 21/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.1253 - accuracy: 0.5850\n",
      "Epoch 21: val_loss did not improve from 1.08598\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1235 - accuracy: 0.5865 - val_loss: 1.1029 - val_accuracy: 0.5939\n",
      "Epoch 22/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.1212 - accuracy: 0.5759\n",
      "Epoch 22: val_loss improved from 1.08598 to 1.07378, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1231 - accuracy: 0.5758 - val_loss: 1.0738 - val_accuracy: 0.6212\n",
      "Epoch 23/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.0949 - accuracy: 0.5924\n",
      "Epoch 23: val_loss improved from 1.07378 to 1.00417, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.0947 - accuracy: 0.5925 - val_loss: 1.0042 - val_accuracy: 0.6419\n",
      "Epoch 24/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.0767 - accuracy: 0.5967\n",
      "Epoch 24: val_loss did not improve from 1.00417\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.0767 - accuracy: 0.5967 - val_loss: 1.0474 - val_accuracy: 0.6092\n",
      "Epoch 25/1000\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 1.0633 - accuracy: 0.6125\n",
      "Epoch 25: val_loss improved from 1.00417 to 0.99825, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.0656 - accuracy: 0.6096 - val_loss: 0.9983 - val_accuracy: 0.6294\n",
      "Epoch 26/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.0508 - accuracy: 0.6135\n",
      "Epoch 26: val_loss improved from 0.99825 to 0.97177, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.0510 - accuracy: 0.6116 - val_loss: 0.9718 - val_accuracy: 0.6561\n",
      "Epoch 27/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.0234 - accuracy: 0.6203\n",
      "Epoch 27: val_loss improved from 0.97177 to 0.92027, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.0261 - accuracy: 0.6198 - val_loss: 0.9203 - val_accuracy: 0.6747\n",
      "Epoch 28/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.0010 - accuracy: 0.6355\n",
      "Epoch 28: val_loss did not improve from 0.92027\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.0010 - accuracy: 0.6355 - val_loss: 0.9849 - val_accuracy: 0.6381\n",
      "Epoch 29/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.9989 - accuracy: 0.6315\n",
      "Epoch 29: val_loss did not improve from 0.92027\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9974 - accuracy: 0.6315 - val_loss: 1.0493 - val_accuracy: 0.6043\n",
      "Epoch 30/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.9820 - accuracy: 0.6390\n",
      "Epoch 30: val_loss did not improve from 0.92027\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9809 - accuracy: 0.6389 - val_loss: 0.9488 - val_accuracy: 0.6583\n",
      "Epoch 31/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.9647 - accuracy: 0.6359\n",
      "Epoch 31: val_loss improved from 0.92027 to 0.88294, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.9744 - accuracy: 0.6320 - val_loss: 0.8829 - val_accuracy: 0.6692\n",
      "Epoch 32/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.9488 - accuracy: 0.6507\n",
      "Epoch 32: val_loss did not improve from 0.88294\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9509 - accuracy: 0.6502 - val_loss: 0.9591 - val_accuracy: 0.6261\n",
      "Epoch 33/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.9388 - accuracy: 0.6482\n",
      "Epoch 33: val_loss improved from 0.88294 to 0.83716, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9350 - accuracy: 0.6495 - val_loss: 0.8372 - val_accuracy: 0.7036\n",
      "Epoch 34/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.9336 - accuracy: 0.6432\n",
      "Epoch 34: val_loss did not improve from 0.83716\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9346 - accuracy: 0.6439 - val_loss: 0.8527 - val_accuracy: 0.6818\n",
      "Epoch 35/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.9191 - accuracy: 0.6551\n",
      "Epoch 35: val_loss improved from 0.83716 to 0.82640, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.9173 - accuracy: 0.6559 - val_loss: 0.8264 - val_accuracy: 0.7014\n",
      "Epoch 36/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.9031 - accuracy: 0.6698\n",
      "Epoch 36: val_loss did not improve from 0.82640\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9089 - accuracy: 0.6673 - val_loss: 0.8855 - val_accuracy: 0.6719\n",
      "Epoch 37/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.8960 - accuracy: 0.6654\n",
      "Epoch 37: val_loss did not improve from 0.82640\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.8956 - accuracy: 0.6659 - val_loss: 0.9575 - val_accuracy: 0.6266\n",
      "Epoch 38/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.8904 - accuracy: 0.6660\n",
      "Epoch 38: val_loss improved from 0.82640 to 0.81438, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.8934 - accuracy: 0.6648 - val_loss: 0.8144 - val_accuracy: 0.6927\n",
      "Epoch 39/1000\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.8803 - accuracy: 0.6742\n",
      "Epoch 39: val_loss did not improve from 0.81438\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.8729 - accuracy: 0.6763 - val_loss: 0.8845 - val_accuracy: 0.6517\n",
      "Epoch 40/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.8724 - accuracy: 0.6752\n",
      "Epoch 40: val_loss improved from 0.81438 to 0.78927, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.8738 - accuracy: 0.6746 - val_loss: 0.7893 - val_accuracy: 0.7118\n",
      "Epoch 41/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.8594 - accuracy: 0.6821\n",
      "Epoch 41: val_loss did not improve from 0.78927\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.8596 - accuracy: 0.6821 - val_loss: 0.8489 - val_accuracy: 0.6883\n",
      "Epoch 42/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.8544 - accuracy: 0.6854\n",
      "Epoch 42: val_loss did not improve from 0.78927\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.8544 - accuracy: 0.6854 - val_loss: 0.8066 - val_accuracy: 0.6867\n",
      "Epoch 43/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.8545 - accuracy: 0.6794\n",
      "Epoch 43: val_loss did not improve from 0.78927\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.8532 - accuracy: 0.6803 - val_loss: 0.8670 - val_accuracy: 0.6632\n",
      "Epoch 44/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.8323 - accuracy: 0.6843\n",
      "Epoch 44: val_loss did not improve from 0.78927\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.8328 - accuracy: 0.6841 - val_loss: 0.8596 - val_accuracy: 0.6627\n",
      "Epoch 45/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.8268 - accuracy: 0.6928\n",
      "Epoch 45: val_loss did not improve from 0.78927\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.8257 - accuracy: 0.6914 - val_loss: 0.8334 - val_accuracy: 0.6758\n",
      "Epoch 46/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.8240 - accuracy: 0.6979\n",
      "Epoch 46: val_loss improved from 0.78927 to 0.74597, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.8221 - accuracy: 0.6981 - val_loss: 0.7460 - val_accuracy: 0.7309\n",
      "Epoch 47/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.8164 - accuracy: 0.6963\n",
      "Epoch 47: val_loss did not improve from 0.74597\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.8152 - accuracy: 0.6963 - val_loss: 0.8438 - val_accuracy: 0.6730\n",
      "Epoch 48/1000\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.8103 - accuracy: 0.6991\n",
      "Epoch 48: val_loss did not improve from 0.74597\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.8129 - accuracy: 0.6990 - val_loss: 0.7602 - val_accuracy: 0.7036\n",
      "Epoch 49/1000\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.7942 - accuracy: 0.7047\n",
      "Epoch 49: val_loss improved from 0.74597 to 0.73692, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7939 - accuracy: 0.7043 - val_loss: 0.7369 - val_accuracy: 0.7260\n",
      "Epoch 50/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.8004 - accuracy: 0.7037\n",
      "Epoch 50: val_loss did not improve from 0.73692\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.8014 - accuracy: 0.7026 - val_loss: 0.7509 - val_accuracy: 0.7172\n",
      "Epoch 51/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.7929 - accuracy: 0.6994\n",
      "Epoch 51: val_loss did not improve from 0.73692\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7954 - accuracy: 0.6986 - val_loss: 0.8736 - val_accuracy: 0.6534\n",
      "Epoch 52/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.7884 - accuracy: 0.7050\n",
      "Epoch 52: val_loss improved from 0.73692 to 0.73087, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.7931 - accuracy: 0.7045 - val_loss: 0.7309 - val_accuracy: 0.7298\n",
      "Epoch 53/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.7796 - accuracy: 0.7156\n",
      "Epoch 53: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7796 - accuracy: 0.7156 - val_loss: 0.8906 - val_accuracy: 0.6419\n",
      "Epoch 54/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.7734 - accuracy: 0.7067\n",
      "Epoch 54: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7727 - accuracy: 0.7077 - val_loss: 0.8210 - val_accuracy: 0.6845\n",
      "Epoch 55/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.7764 - accuracy: 0.7042\n",
      "Epoch 55: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.7761 - accuracy: 0.7048 - val_loss: 0.7963 - val_accuracy: 0.6785\n",
      "Epoch 56/1000\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.7758 - accuracy: 0.7097\n",
      "Epoch 56: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.7731 - accuracy: 0.7106 - val_loss: 0.8295 - val_accuracy: 0.6730\n",
      "Epoch 57/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.7634 - accuracy: 0.7182\n",
      "Epoch 57: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7649 - accuracy: 0.7170 - val_loss: 0.8077 - val_accuracy: 0.6790\n",
      "Epoch 58/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.7557 - accuracy: 0.7136\n",
      "Epoch 58: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7517 - accuracy: 0.7154 - val_loss: 0.7985 - val_accuracy: 0.6818\n",
      "Epoch 59/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.7410 - accuracy: 0.7215\n",
      "Epoch 59: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7400 - accuracy: 0.7217 - val_loss: 0.8255 - val_accuracy: 0.6676\n",
      "Epoch 60/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.7410 - accuracy: 0.7240\n",
      "Epoch 60: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7425 - accuracy: 0.7236 - val_loss: 0.7481 - val_accuracy: 0.7189\n",
      "Epoch 61/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.7330 - accuracy: 0.7197\n",
      "Epoch 61: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7304 - accuracy: 0.7207 - val_loss: 0.8079 - val_accuracy: 0.6692\n",
      "Epoch 62/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.7342 - accuracy: 0.7281\n",
      "Epoch 62: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7348 - accuracy: 0.7283 - val_loss: 0.9697 - val_accuracy: 0.6206\n",
      "Epoch 63/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.7403 - accuracy: 0.7257\n",
      "Epoch 63: val_loss did not improve from 0.73087\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7390 - accuracy: 0.7261 - val_loss: 0.8519 - val_accuracy: 0.6599\n",
      "Epoch 64/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.7255 - accuracy: 0.7232\n",
      "Epoch 64: val_loss improved from 0.73087 to 0.70717, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.7236 - accuracy: 0.7237 - val_loss: 0.7072 - val_accuracy: 0.7140\n",
      "Epoch 65/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.7151 - accuracy: 0.7304\n",
      "Epoch 65: val_loss improved from 0.70717 to 0.67248, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7133 - accuracy: 0.7301 - val_loss: 0.6725 - val_accuracy: 0.7462\n",
      "Epoch 66/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.7379\n",
      "Epoch 66: val_loss did not improve from 0.67248\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7061 - accuracy: 0.7379 - val_loss: 0.7694 - val_accuracy: 0.6878\n",
      "Epoch 67/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.7087 - accuracy: 0.7404\n",
      "Epoch 67: val_loss did not improve from 0.67248\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7072 - accuracy: 0.7412 - val_loss: 0.7738 - val_accuracy: 0.6992\n",
      "Epoch 68/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6943 - accuracy: 0.7351\n",
      "Epoch 68: val_loss improved from 0.67248 to 0.63762, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6946 - accuracy: 0.7361 - val_loss: 0.6376 - val_accuracy: 0.7538\n",
      "Epoch 69/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.7467\n",
      "Epoch 69: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6967 - accuracy: 0.7467 - val_loss: 0.8163 - val_accuracy: 0.6670\n",
      "Epoch 70/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6980 - accuracy: 0.7428\n",
      "Epoch 70: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6963 - accuracy: 0.7430 - val_loss: 0.6717 - val_accuracy: 0.7369\n",
      "Epoch 71/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6828 - accuracy: 0.7461\n",
      "Epoch 71: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6832 - accuracy: 0.7461 - val_loss: 0.7880 - val_accuracy: 0.6725\n",
      "Epoch 72/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.7492\n",
      "Epoch 72: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6916 - accuracy: 0.7492 - val_loss: 0.9031 - val_accuracy: 0.6457\n",
      "Epoch 73/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6823 - accuracy: 0.7506\n",
      "Epoch 73: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6810 - accuracy: 0.7503 - val_loss: 0.7404 - val_accuracy: 0.7096\n",
      "Epoch 74/1000\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.6789 - accuracy: 0.7517\n",
      "Epoch 74: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6777 - accuracy: 0.7507 - val_loss: 0.6818 - val_accuracy: 0.7385\n",
      "Epoch 75/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.6750 - accuracy: 0.7552\n",
      "Epoch 75: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6761 - accuracy: 0.7551 - val_loss: 0.6777 - val_accuracy: 0.7549\n",
      "Epoch 76/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.7427\n",
      "Epoch 76: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6830 - accuracy: 0.7427 - val_loss: 0.7682 - val_accuracy: 0.6981\n",
      "Epoch 77/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6692 - accuracy: 0.7515\n",
      "Epoch 77: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6691 - accuracy: 0.7512 - val_loss: 0.6563 - val_accuracy: 0.7587\n",
      "Epoch 78/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6600 - accuracy: 0.7553\n",
      "Epoch 78: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6597 - accuracy: 0.7551 - val_loss: 0.6829 - val_accuracy: 0.7462\n",
      "Epoch 79/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.6591 - accuracy: 0.7592\n",
      "Epoch 79: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6591 - accuracy: 0.7592 - val_loss: 0.8588 - val_accuracy: 0.6425\n",
      "Epoch 80/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6640 - accuracy: 0.7521\n",
      "Epoch 80: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6645 - accuracy: 0.7512 - val_loss: 0.9430 - val_accuracy: 0.6119\n",
      "Epoch 81/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.6561 - accuracy: 0.7595\n",
      "Epoch 81: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6554 - accuracy: 0.7589 - val_loss: 0.7668 - val_accuracy: 0.6823\n",
      "Epoch 82/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6527 - accuracy: 0.7561\n",
      "Epoch 82: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6533 - accuracy: 0.7554 - val_loss: 0.6419 - val_accuracy: 0.7451\n",
      "Epoch 83/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6585 - accuracy: 0.7570\n",
      "Epoch 83: val_loss did not improve from 0.63762\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6563 - accuracy: 0.7574 - val_loss: 0.7093 - val_accuracy: 0.7178\n",
      "Epoch 84/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.6443 - accuracy: 0.7558\n",
      "Epoch 84: val_loss improved from 0.63762 to 0.63642, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6443 - accuracy: 0.7558 - val_loss: 0.6364 - val_accuracy: 0.7495\n",
      "Epoch 85/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6341 - accuracy: 0.7673\n",
      "Epoch 85: val_loss did not improve from 0.63642\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6367 - accuracy: 0.7667 - val_loss: 0.7588 - val_accuracy: 0.6900\n",
      "Epoch 86/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.6385 - accuracy: 0.7717\n",
      "Epoch 86: val_loss did not improve from 0.63642\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6401 - accuracy: 0.7705 - val_loss: 0.6758 - val_accuracy: 0.7380\n",
      "Epoch 87/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6306 - accuracy: 0.7706\n",
      "Epoch 87: val_loss did not improve from 0.63642\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6300 - accuracy: 0.7714 - val_loss: 0.6655 - val_accuracy: 0.7342\n",
      "Epoch 88/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6283 - accuracy: 0.7720\n",
      "Epoch 88: val_loss did not improve from 0.63642\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6280 - accuracy: 0.7725 - val_loss: 0.6757 - val_accuracy: 0.7271\n",
      "Epoch 89/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6265 - accuracy: 0.7724\n",
      "Epoch 89: val_loss did not improve from 0.63642\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6251 - accuracy: 0.7729 - val_loss: 0.7788 - val_accuracy: 0.6801\n",
      "Epoch 90/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.7732\n",
      "Epoch 90: val_loss did not improve from 0.63642\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6300 - accuracy: 0.7732 - val_loss: 0.7561 - val_accuracy: 0.7052\n",
      "Epoch 91/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6301 - accuracy: 0.7642\n",
      "Epoch 91: val_loss did not improve from 0.63642\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6281 - accuracy: 0.7647 - val_loss: 0.8194 - val_accuracy: 0.6719\n",
      "Epoch 92/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.6348 - accuracy: 0.7666\n",
      "Epoch 92: val_loss improved from 0.63642 to 0.63055, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6336 - accuracy: 0.7671 - val_loss: 0.6306 - val_accuracy: 0.7555\n",
      "Epoch 93/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.6121 - accuracy: 0.7759\n",
      "Epoch 93: val_loss did not improve from 0.63055\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6118 - accuracy: 0.7773 - val_loss: 0.7169 - val_accuracy: 0.7031\n",
      "Epoch 94/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.6163 - accuracy: 0.7729\n",
      "Epoch 94: val_loss did not improve from 0.63055\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6185 - accuracy: 0.7722 - val_loss: 0.6335 - val_accuracy: 0.7489\n",
      "Epoch 95/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6098 - accuracy: 0.7801\n",
      "Epoch 95: val_loss did not improve from 0.63055\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6072 - accuracy: 0.7813 - val_loss: 0.7847 - val_accuracy: 0.6829\n",
      "Epoch 96/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.6123 - accuracy: 0.7803\n",
      "Epoch 96: val_loss did not improve from 0.63055\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6110 - accuracy: 0.7805 - val_loss: 0.7028 - val_accuracy: 0.7200\n",
      "Epoch 97/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.6100 - accuracy: 0.7745\n",
      "Epoch 97: val_loss did not improve from 0.63055\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6100 - accuracy: 0.7745 - val_loss: 0.6873 - val_accuracy: 0.7222\n",
      "Epoch 98/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.6026 - accuracy: 0.7826\n",
      "Epoch 98: val_loss did not improve from 0.63055\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5958 - accuracy: 0.7849 - val_loss: 0.6489 - val_accuracy: 0.7396\n",
      "Epoch 99/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5958 - accuracy: 0.7838\n",
      "Epoch 99: val_loss did not improve from 0.63055\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5969 - accuracy: 0.7831 - val_loss: 0.6518 - val_accuracy: 0.7484\n",
      "Epoch 100/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5947 - accuracy: 0.7849\n",
      "Epoch 100: val_loss improved from 0.63055 to 0.61783, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5947 - accuracy: 0.7849 - val_loss: 0.6178 - val_accuracy: 0.7593\n",
      "Epoch 101/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5870 - accuracy: 0.7879\n",
      "Epoch 101: val_loss did not improve from 0.61783\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5855 - accuracy: 0.7882 - val_loss: 0.7430 - val_accuracy: 0.7020\n",
      "Epoch 102/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5901 - accuracy: 0.7829\n",
      "Epoch 102: val_loss did not improve from 0.61783\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5901 - accuracy: 0.7829 - val_loss: 0.6447 - val_accuracy: 0.7364\n",
      "Epoch 103/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5981 - accuracy: 0.7789\n",
      "Epoch 103: val_loss did not improve from 0.61783\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5961 - accuracy: 0.7800 - val_loss: 0.7907 - val_accuracy: 0.6829\n",
      "Epoch 104/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.5863 - accuracy: 0.7872\n",
      "Epoch 104: val_loss did not improve from 0.61783\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5854 - accuracy: 0.7865 - val_loss: 0.6369 - val_accuracy: 0.7522\n",
      "Epoch 105/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5845 - accuracy: 0.7873\n",
      "Epoch 105: val_loss did not improve from 0.61783\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5851 - accuracy: 0.7869 - val_loss: 0.6940 - val_accuracy: 0.7233\n",
      "Epoch 106/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.7925\n",
      "Epoch 106: val_loss did not improve from 0.61783\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5738 - accuracy: 0.7925 - val_loss: 0.7537 - val_accuracy: 0.6949\n",
      "Epoch 107/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5746 - accuracy: 0.7924\n",
      "Epoch 107: val_loss improved from 0.61783 to 0.61027, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5722 - accuracy: 0.7925 - val_loss: 0.6103 - val_accuracy: 0.7615\n",
      "Epoch 108/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5771 - accuracy: 0.7913\n",
      "Epoch 108: val_loss did not improve from 0.61027\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5754 - accuracy: 0.7904 - val_loss: 0.8253 - val_accuracy: 0.6698\n",
      "Epoch 109/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5883 - accuracy: 0.7807\n",
      "Epoch 109: val_loss did not improve from 0.61027\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5874 - accuracy: 0.7820 - val_loss: 0.8183 - val_accuracy: 0.6779\n",
      "Epoch 110/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.7900\n",
      "Epoch 110: val_loss did not improve from 0.61027\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5752 - accuracy: 0.7900 - val_loss: 0.7404 - val_accuracy: 0.6971\n",
      "Epoch 111/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5729 - accuracy: 0.7903\n",
      "Epoch 111: val_loss did not improve from 0.61027\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5736 - accuracy: 0.7893 - val_loss: 0.6908 - val_accuracy: 0.7172\n",
      "Epoch 112/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5768 - accuracy: 0.7912\n",
      "Epoch 112: val_loss did not improve from 0.61027\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5784 - accuracy: 0.7905 - val_loss: 0.8441 - val_accuracy: 0.6654\n",
      "Epoch 113/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7967\n",
      "Epoch 113: val_loss improved from 0.61027 to 0.56056, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5641 - accuracy: 0.7960 - val_loss: 0.5606 - val_accuracy: 0.7980\n",
      "Epoch 114/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5749 - accuracy: 0.7929\n",
      "Epoch 114: val_loss did not improve from 0.56056\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5769 - accuracy: 0.7925 - val_loss: 0.6614 - val_accuracy: 0.7364\n",
      "Epoch 115/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7888\n",
      "Epoch 115: val_loss did not improve from 0.56056\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5712 - accuracy: 0.7889 - val_loss: 0.7212 - val_accuracy: 0.7167\n",
      "Epoch 116/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5694 - accuracy: 0.7915\n",
      "Epoch 116: val_loss improved from 0.56056 to 0.54639, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5693 - accuracy: 0.7918 - val_loss: 0.5464 - val_accuracy: 0.7926\n",
      "Epoch 117/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7950\n",
      "Epoch 117: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5596 - accuracy: 0.7958 - val_loss: 0.7131 - val_accuracy: 0.7112\n",
      "Epoch 118/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5561 - accuracy: 0.7936\n",
      "Epoch 118: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5549 - accuracy: 0.7940 - val_loss: 0.6308 - val_accuracy: 0.7489\n",
      "Epoch 119/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7954\n",
      "Epoch 119: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5635 - accuracy: 0.7964 - val_loss: 0.6456 - val_accuracy: 0.7473\n",
      "Epoch 120/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.5518 - accuracy: 0.8041\n",
      "Epoch 120: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5501 - accuracy: 0.8049 - val_loss: 0.5759 - val_accuracy: 0.7855\n",
      "Epoch 121/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5598 - accuracy: 0.8062\n",
      "Epoch 121: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5592 - accuracy: 0.8055 - val_loss: 0.6907 - val_accuracy: 0.7298\n",
      "Epoch 122/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5661 - accuracy: 0.7939\n",
      "Epoch 122: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5687 - accuracy: 0.7929 - val_loss: 0.6181 - val_accuracy: 0.7560\n",
      "Epoch 123/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.8027\n",
      "Epoch 123: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5494 - accuracy: 0.8027 - val_loss: 0.7085 - val_accuracy: 0.7205\n",
      "Epoch 124/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.7955\n",
      "Epoch 124: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5566 - accuracy: 0.7955 - val_loss: 0.5769 - val_accuracy: 0.7866\n",
      "Epoch 125/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.8093\n",
      "Epoch 125: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5467 - accuracy: 0.8093 - val_loss: 0.7009 - val_accuracy: 0.7172\n",
      "Epoch 126/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5506 - accuracy: 0.8059\n",
      "Epoch 126: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5506 - accuracy: 0.8051 - val_loss: 0.5771 - val_accuracy: 0.7778\n",
      "Epoch 127/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.5383 - accuracy: 0.8090\n",
      "Epoch 127: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5386 - accuracy: 0.8089 - val_loss: 0.6886 - val_accuracy: 0.7347\n",
      "Epoch 128/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.8058\n",
      "Epoch 128: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5411 - accuracy: 0.8058 - val_loss: 0.7342 - val_accuracy: 0.7003\n",
      "Epoch 129/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.5373 - accuracy: 0.8051\n",
      "Epoch 129: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5360 - accuracy: 0.8060 - val_loss: 0.5609 - val_accuracy: 0.7866\n",
      "Epoch 130/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5292 - accuracy: 0.8074\n",
      "Epoch 130: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5339 - accuracy: 0.8055 - val_loss: 0.7422 - val_accuracy: 0.7091\n",
      "Epoch 131/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5568 - accuracy: 0.7973\n",
      "Epoch 131: val_loss did not improve from 0.54639\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5567 - accuracy: 0.7967 - val_loss: 0.6557 - val_accuracy: 0.7429\n",
      "Epoch 132/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5250 - accuracy: 0.8110\n",
      "Epoch 132: val_loss improved from 0.54639 to 0.54509, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5241 - accuracy: 0.8115 - val_loss: 0.5451 - val_accuracy: 0.7991\n",
      "Epoch 133/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5333 - accuracy: 0.8046\n",
      "Epoch 133: val_loss improved from 0.54509 to 0.51046, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5335 - accuracy: 0.8045 - val_loss: 0.5105 - val_accuracy: 0.8171\n",
      "Epoch 134/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5311 - accuracy: 0.8114\n",
      "Epoch 134: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5296 - accuracy: 0.8120 - val_loss: 0.7243 - val_accuracy: 0.7178\n",
      "Epoch 135/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.8055\n",
      "Epoch 135: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5378 - accuracy: 0.8055 - val_loss: 0.6943 - val_accuracy: 0.7276\n",
      "Epoch 136/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5208 - accuracy: 0.8144\n",
      "Epoch 136: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5208 - accuracy: 0.8144 - val_loss: 0.7115 - val_accuracy: 0.7183\n",
      "Epoch 137/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5295 - accuracy: 0.8074\n",
      "Epoch 137: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5315 - accuracy: 0.8060 - val_loss: 0.7721 - val_accuracy: 0.7047\n",
      "Epoch 138/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5264 - accuracy: 0.8150\n",
      "Epoch 138: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5299 - accuracy: 0.8122 - val_loss: 0.7499 - val_accuracy: 0.6992\n",
      "Epoch 139/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5296 - accuracy: 0.8100\n",
      "Epoch 139: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5306 - accuracy: 0.8091 - val_loss: 0.6412 - val_accuracy: 0.7473\n",
      "Epoch 140/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5288 - accuracy: 0.8121\n",
      "Epoch 140: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5287 - accuracy: 0.8120 - val_loss: 0.7239 - val_accuracy: 0.7222\n",
      "Epoch 141/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.5276 - accuracy: 0.8086\n",
      "Epoch 141: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5267 - accuracy: 0.8095 - val_loss: 0.5363 - val_accuracy: 0.7986\n",
      "Epoch 142/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5223 - accuracy: 0.8085\n",
      "Epoch 142: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5276 - accuracy: 0.8078 - val_loss: 0.6566 - val_accuracy: 0.7462\n",
      "Epoch 143/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5221 - accuracy: 0.8119\n",
      "Epoch 143: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5212 - accuracy: 0.8120 - val_loss: 0.5483 - val_accuracy: 0.8008\n",
      "Epoch 144/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.8164\n",
      "Epoch 144: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5133 - accuracy: 0.8166 - val_loss: 0.5813 - val_accuracy: 0.7778\n",
      "Epoch 145/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.8129\n",
      "Epoch 145: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5292 - accuracy: 0.8129 - val_loss: 0.5558 - val_accuracy: 0.7926\n",
      "Epoch 146/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.8222\n",
      "Epoch 146: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5041 - accuracy: 0.8222 - val_loss: 0.6061 - val_accuracy: 0.7697\n",
      "Epoch 147/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.8116\n",
      "Epoch 147: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5173 - accuracy: 0.8116 - val_loss: 0.6155 - val_accuracy: 0.7653\n",
      "Epoch 148/1000\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.5212 - accuracy: 0.8152\n",
      "Epoch 148: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5191 - accuracy: 0.8160 - val_loss: 0.5315 - val_accuracy: 0.8062\n",
      "Epoch 149/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5098 - accuracy: 0.8187\n",
      "Epoch 149: val_loss did not improve from 0.51046\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5103 - accuracy: 0.8177 - val_loss: 0.8277 - val_accuracy: 0.6850\n",
      "Epoch 150/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5154 - accuracy: 0.8118\n",
      "Epoch 150: val_loss improved from 0.51046 to 0.50665, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5154 - accuracy: 0.8118 - val_loss: 0.5067 - val_accuracy: 0.8150\n",
      "Epoch 151/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4967 - accuracy: 0.8229\n",
      "Epoch 151: val_loss did not improve from 0.50665\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4967 - accuracy: 0.8229 - val_loss: 0.7304 - val_accuracy: 0.7167\n",
      "Epoch 152/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5097 - accuracy: 0.8176\n",
      "Epoch 152: val_loss did not improve from 0.50665\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5071 - accuracy: 0.8189 - val_loss: 0.5681 - val_accuracy: 0.7898\n",
      "Epoch 153/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4922 - accuracy: 0.8306\n",
      "Epoch 153: val_loss did not improve from 0.50665\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4922 - accuracy: 0.8306 - val_loss: 0.5136 - val_accuracy: 0.8106\n",
      "Epoch 154/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.8237\n",
      "Epoch 154: val_loss improved from 0.50665 to 0.46318, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4991 - accuracy: 0.8237 - val_loss: 0.4632 - val_accuracy: 0.8422\n",
      "Epoch 155/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5061 - accuracy: 0.8218\n",
      "Epoch 155: val_loss did not improve from 0.46318\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5061 - accuracy: 0.8218 - val_loss: 0.5584 - val_accuracy: 0.7904\n",
      "Epoch 156/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.8144\n",
      "Epoch 156: val_loss did not improve from 0.46318\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5139 - accuracy: 0.8149 - val_loss: 0.4928 - val_accuracy: 0.8215\n",
      "Epoch 157/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.8222\n",
      "Epoch 157: val_loss did not improve from 0.46318\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5051 - accuracy: 0.8224 - val_loss: 0.7971 - val_accuracy: 0.6987\n",
      "Epoch 158/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.8197\n",
      "Epoch 158: val_loss improved from 0.46318 to 0.43981, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5038 - accuracy: 0.8197 - val_loss: 0.4398 - val_accuracy: 0.8499\n",
      "Epoch 159/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4951 - accuracy: 0.8198\n",
      "Epoch 159: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4950 - accuracy: 0.8198 - val_loss: 0.5381 - val_accuracy: 0.7975\n",
      "Epoch 160/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4943 - accuracy: 0.8230\n",
      "Epoch 160: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4943 - accuracy: 0.8229 - val_loss: 0.5357 - val_accuracy: 0.8013\n",
      "Epoch 161/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5076 - accuracy: 0.8129\n",
      "Epoch 161: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5085 - accuracy: 0.8124 - val_loss: 0.7333 - val_accuracy: 0.7293\n",
      "Epoch 162/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4857 - accuracy: 0.8270\n",
      "Epoch 162: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4861 - accuracy: 0.8275 - val_loss: 0.7601 - val_accuracy: 0.7096\n",
      "Epoch 163/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4951 - accuracy: 0.8249\n",
      "Epoch 163: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4964 - accuracy: 0.8249 - val_loss: 0.5926 - val_accuracy: 0.7833\n",
      "Epoch 164/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5021 - accuracy: 0.8195\n",
      "Epoch 164: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5021 - accuracy: 0.8195 - val_loss: 0.7079 - val_accuracy: 0.7260\n",
      "Epoch 165/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.8344\n",
      "Epoch 165: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4809 - accuracy: 0.8355 - val_loss: 0.5815 - val_accuracy: 0.7795\n",
      "Epoch 166/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4875 - accuracy: 0.8271\n",
      "Epoch 166: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4875 - accuracy: 0.8271 - val_loss: 0.5151 - val_accuracy: 0.8100\n",
      "Epoch 167/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4903 - accuracy: 0.8232\n",
      "Epoch 167: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4904 - accuracy: 0.8231 - val_loss: 0.4904 - val_accuracy: 0.8177\n",
      "Epoch 168/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4867 - accuracy: 0.8271\n",
      "Epoch 168: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4867 - accuracy: 0.8271 - val_loss: 0.5388 - val_accuracy: 0.8046\n",
      "Epoch 169/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4818 - accuracy: 0.8256\n",
      "Epoch 169: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4821 - accuracy: 0.8255 - val_loss: 0.6379 - val_accuracy: 0.7615\n",
      "Epoch 170/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4807 - accuracy: 0.8268\n",
      "Epoch 170: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4820 - accuracy: 0.8262 - val_loss: 0.6360 - val_accuracy: 0.7669\n",
      "Epoch 171/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4820 - accuracy: 0.8263\n",
      "Epoch 171: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4846 - accuracy: 0.8260 - val_loss: 0.5687 - val_accuracy: 0.7860\n",
      "Epoch 172/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.8271\n",
      "Epoch 172: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4810 - accuracy: 0.8271 - val_loss: 0.7111 - val_accuracy: 0.7314\n",
      "Epoch 173/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4773 - accuracy: 0.8275\n",
      "Epoch 173: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4773 - accuracy: 0.8275 - val_loss: 0.4860 - val_accuracy: 0.8253\n",
      "Epoch 174/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4810 - accuracy: 0.8276\n",
      "Epoch 174: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.4776 - accuracy: 0.8295 - val_loss: 0.6153 - val_accuracy: 0.7757\n",
      "Epoch 175/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4890 - accuracy: 0.8209\n",
      "Epoch 175: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4901 - accuracy: 0.8202 - val_loss: 0.5403 - val_accuracy: 0.8013\n",
      "Epoch 176/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4709 - accuracy: 0.8333\n",
      "Epoch 176: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4714 - accuracy: 0.8326 - val_loss: 0.6181 - val_accuracy: 0.7691\n",
      "Epoch 177/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4654 - accuracy: 0.8387\n",
      "Epoch 177: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4647 - accuracy: 0.8384 - val_loss: 0.5617 - val_accuracy: 0.7997\n",
      "Epoch 178/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.8224\n",
      "Epoch 178: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4782 - accuracy: 0.8224 - val_loss: 0.5823 - val_accuracy: 0.7833\n",
      "Epoch 179/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4633 - accuracy: 0.8329\n",
      "Epoch 179: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4647 - accuracy: 0.8318 - val_loss: 0.4818 - val_accuracy: 0.8319\n",
      "Epoch 180/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.8349\n",
      "Epoch 180: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.4663 - accuracy: 0.8349 - val_loss: 0.5184 - val_accuracy: 0.8002\n",
      "Epoch 181/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4949 - accuracy: 0.8181\n",
      "Epoch 181: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4952 - accuracy: 0.8186 - val_loss: 0.8058 - val_accuracy: 0.6927\n",
      "Epoch 182/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4835 - accuracy: 0.8308\n",
      "Epoch 182: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4808 - accuracy: 0.8318 - val_loss: 0.4855 - val_accuracy: 0.8221\n",
      "Epoch 183/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.8286\n",
      "Epoch 183: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4802 - accuracy: 0.8286 - val_loss: 0.5218 - val_accuracy: 0.8155\n",
      "Epoch 184/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4868 - accuracy: 0.8283\n",
      "Epoch 184: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4872 - accuracy: 0.8286 - val_loss: 0.5880 - val_accuracy: 0.7740\n",
      "Epoch 185/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.8264\n",
      "Epoch 185: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4810 - accuracy: 0.8264 - val_loss: 0.5220 - val_accuracy: 0.8090\n",
      "Epoch 186/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4820 - accuracy: 0.8272\n",
      "Epoch 186: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4819 - accuracy: 0.8277 - val_loss: 0.4921 - val_accuracy: 0.8177\n",
      "Epoch 187/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4565 - accuracy: 0.8369\n",
      "Epoch 187: val_loss did not improve from 0.43981\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4555 - accuracy: 0.8371 - val_loss: 0.5833 - val_accuracy: 0.7871\n",
      "Epoch 188/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4757 - accuracy: 0.8343\n",
      "Epoch 188: val_loss improved from 0.43981 to 0.43307, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4738 - accuracy: 0.8342 - val_loss: 0.4331 - val_accuracy: 0.8483\n",
      "Epoch 189/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4564 - accuracy: 0.8377\n",
      "Epoch 189: val_loss did not improve from 0.43307\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4584 - accuracy: 0.8379 - val_loss: 0.6213 - val_accuracy: 0.7735\n",
      "Epoch 190/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.8415\n",
      "Epoch 190: val_loss did not improve from 0.43307\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4597 - accuracy: 0.8415 - val_loss: 0.6315 - val_accuracy: 0.7571\n",
      "Epoch 191/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.8360\n",
      "Epoch 191: val_loss did not improve from 0.43307\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4610 - accuracy: 0.8360 - val_loss: 0.8279 - val_accuracy: 0.7031\n",
      "Epoch 192/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4826 - accuracy: 0.8265\n",
      "Epoch 192: val_loss did not improve from 0.43307\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4815 - accuracy: 0.8268 - val_loss: 0.5038 - val_accuracy: 0.8171\n",
      "Epoch 193/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4552 - accuracy: 0.8386\n",
      "Epoch 193: val_loss did not improve from 0.43307\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4540 - accuracy: 0.8388 - val_loss: 0.5716 - val_accuracy: 0.7920\n",
      "Epoch 194/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4689 - accuracy: 0.8277\n",
      "Epoch 194: val_loss improved from 0.43307 to 0.42224, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4714 - accuracy: 0.8260 - val_loss: 0.4222 - val_accuracy: 0.8543\n",
      "Epoch 195/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4638 - accuracy: 0.8352\n",
      "Epoch 195: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4643 - accuracy: 0.8348 - val_loss: 0.6177 - val_accuracy: 0.7713\n",
      "Epoch 196/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.8348\n",
      "Epoch 196: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4635 - accuracy: 0.8348 - val_loss: 0.4230 - val_accuracy: 0.8564\n",
      "Epoch 197/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.8464\n",
      "Epoch 197: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4447 - accuracy: 0.8464 - val_loss: 0.4726 - val_accuracy: 0.8352\n",
      "Epoch 198/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4629 - accuracy: 0.8341\n",
      "Epoch 198: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4618 - accuracy: 0.8351 - val_loss: 0.5525 - val_accuracy: 0.7904\n",
      "Epoch 199/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4578 - accuracy: 0.8413\n",
      "Epoch 199: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4596 - accuracy: 0.8408 - val_loss: 0.5710 - val_accuracy: 0.7893\n",
      "Epoch 200/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4426 - accuracy: 0.8419\n",
      "Epoch 200: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4408 - accuracy: 0.8426 - val_loss: 0.4853 - val_accuracy: 0.8259\n",
      "Epoch 201/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4533 - accuracy: 0.8353\n",
      "Epoch 201: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4531 - accuracy: 0.8355 - val_loss: 0.4257 - val_accuracy: 0.8521\n",
      "Epoch 202/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4456 - accuracy: 0.8389\n",
      "Epoch 202: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4458 - accuracy: 0.8389 - val_loss: 0.5282 - val_accuracy: 0.8062\n",
      "Epoch 203/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.8435\n",
      "Epoch 203: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4416 - accuracy: 0.8435 - val_loss: 0.5761 - val_accuracy: 0.7800\n",
      "Epoch 204/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4386 - accuracy: 0.8567\n",
      "Epoch 204: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4402 - accuracy: 0.8571 - val_loss: 0.5427 - val_accuracy: 0.8051\n",
      "Epoch 205/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4598 - accuracy: 0.8352\n",
      "Epoch 205: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4588 - accuracy: 0.8344 - val_loss: 0.5261 - val_accuracy: 0.8079\n",
      "Epoch 206/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.8415\n",
      "Epoch 206: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4457 - accuracy: 0.8415 - val_loss: 0.6237 - val_accuracy: 0.7691\n",
      "Epoch 207/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8424\n",
      "Epoch 207: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4469 - accuracy: 0.8424 - val_loss: 0.4875 - val_accuracy: 0.8253\n",
      "Epoch 208/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4473 - accuracy: 0.8426\n",
      "Epoch 208: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4473 - accuracy: 0.8426 - val_loss: 0.5944 - val_accuracy: 0.7762\n",
      "Epoch 209/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4407 - accuracy: 0.8510\n",
      "Epoch 209: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4461 - accuracy: 0.8495 - val_loss: 0.5015 - val_accuracy: 0.8248\n",
      "Epoch 210/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4548 - accuracy: 0.8409\n",
      "Epoch 210: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4522 - accuracy: 0.8429 - val_loss: 0.5532 - val_accuracy: 0.8084\n",
      "Epoch 211/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.8367\n",
      "Epoch 211: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4427 - accuracy: 0.8368 - val_loss: 0.6118 - val_accuracy: 0.7773\n",
      "Epoch 212/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.8461\n",
      "Epoch 212: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4399 - accuracy: 0.8457 - val_loss: 0.5546 - val_accuracy: 0.8040\n",
      "Epoch 213/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.8452\n",
      "Epoch 213: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4358 - accuracy: 0.8442 - val_loss: 0.5439 - val_accuracy: 0.8019\n",
      "Epoch 214/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4334 - accuracy: 0.8494\n",
      "Epoch 214: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4320 - accuracy: 0.8499 - val_loss: 0.5005 - val_accuracy: 0.8204\n",
      "Epoch 215/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4443 - accuracy: 0.8404\n",
      "Epoch 215: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4456 - accuracy: 0.8399 - val_loss: 0.5673 - val_accuracy: 0.7882\n",
      "Epoch 216/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4330 - accuracy: 0.8473\n",
      "Epoch 216: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4325 - accuracy: 0.8488 - val_loss: 0.6383 - val_accuracy: 0.7686\n",
      "Epoch 217/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4411 - accuracy: 0.8424\n",
      "Epoch 217: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4400 - accuracy: 0.8426 - val_loss: 0.6254 - val_accuracy: 0.7626\n",
      "Epoch 218/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8494\n",
      "Epoch 218: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4269 - accuracy: 0.8495 - val_loss: 0.5208 - val_accuracy: 0.8029\n",
      "Epoch 219/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4456 - accuracy: 0.8467\n",
      "Epoch 219: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 0.4428 - accuracy: 0.8480 - val_loss: 0.6138 - val_accuracy: 0.7675\n",
      "Epoch 220/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4434 - accuracy: 0.8434\n",
      "Epoch 220: val_loss did not improve from 0.42224\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4470 - accuracy: 0.8422 - val_loss: 0.5982 - val_accuracy: 0.7838\n",
      "Epoch 221/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4187 - accuracy: 0.8552\n",
      "Epoch 221: val_loss improved from 0.42224 to 0.42155, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4206 - accuracy: 0.8542 - val_loss: 0.4215 - val_accuracy: 0.8521\n",
      "Epoch 222/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8564\n",
      "Epoch 222: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4182 - accuracy: 0.8564 - val_loss: 0.4969 - val_accuracy: 0.8210\n",
      "Epoch 223/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4417 - accuracy: 0.8460\n",
      "Epoch 223: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4417 - accuracy: 0.8460 - val_loss: 0.4747 - val_accuracy: 0.8297\n",
      "Epoch 224/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.8366\n",
      "Epoch 224: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4432 - accuracy: 0.8373 - val_loss: 0.5361 - val_accuracy: 0.8024\n",
      "Epoch 225/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4225 - accuracy: 0.8571\n",
      "Epoch 225: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4205 - accuracy: 0.8575 - val_loss: 0.4574 - val_accuracy: 0.8439\n",
      "Epoch 226/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4238 - accuracy: 0.8560\n",
      "Epoch 226: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4234 - accuracy: 0.8553 - val_loss: 0.4487 - val_accuracy: 0.8493\n",
      "Epoch 227/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4276 - accuracy: 0.8422\n",
      "Epoch 227: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4301 - accuracy: 0.8411 - val_loss: 0.4899 - val_accuracy: 0.8226\n",
      "Epoch 228/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8522\n",
      "Epoch 228: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.4316 - accuracy: 0.8522 - val_loss: 0.6273 - val_accuracy: 0.7724\n",
      "Epoch 229/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8553\n",
      "Epoch 229: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4179 - accuracy: 0.8553 - val_loss: 0.4485 - val_accuracy: 0.8412\n",
      "Epoch 230/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.8539\n",
      "Epoch 230: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4220 - accuracy: 0.8539 - val_loss: 0.4289 - val_accuracy: 0.8597\n",
      "Epoch 231/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4222 - accuracy: 0.8514\n",
      "Epoch 231: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4260 - accuracy: 0.8506 - val_loss: 0.5918 - val_accuracy: 0.7904\n",
      "Epoch 232/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8546\n",
      "Epoch 232: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4209 - accuracy: 0.8555 - val_loss: 0.4713 - val_accuracy: 0.8417\n",
      "Epoch 233/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4201 - accuracy: 0.8537\n",
      "Epoch 233: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4211 - accuracy: 0.8522 - val_loss: 0.5322 - val_accuracy: 0.8122\n",
      "Epoch 234/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4131 - accuracy: 0.8607\n",
      "Epoch 234: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4182 - accuracy: 0.8590 - val_loss: 0.4274 - val_accuracy: 0.8499\n",
      "Epoch 235/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4249 - accuracy: 0.8490\n",
      "Epoch 235: val_loss did not improve from 0.42155\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4240 - accuracy: 0.8502 - val_loss: 0.5256 - val_accuracy: 0.8106\n",
      "Epoch 236/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.8539\n",
      "Epoch 236: val_loss improved from 0.42155 to 0.40104, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4210 - accuracy: 0.8539 - val_loss: 0.4010 - val_accuracy: 0.8663\n",
      "Epoch 237/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4171 - accuracy: 0.8502\n",
      "Epoch 237: val_loss did not improve from 0.40104\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4151 - accuracy: 0.8502 - val_loss: 0.5193 - val_accuracy: 0.8046\n",
      "Epoch 238/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4086 - accuracy: 0.8561\n",
      "Epoch 238: val_loss did not improve from 0.40104\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4056 - accuracy: 0.8571 - val_loss: 0.4398 - val_accuracy: 0.8483\n",
      "Epoch 239/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4011 - accuracy: 0.8628\n",
      "Epoch 239: val_loss did not improve from 0.40104\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4001 - accuracy: 0.8628 - val_loss: 0.4782 - val_accuracy: 0.8248\n",
      "Epoch 240/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.4212 - accuracy: 0.8504\n",
      "Epoch 240: val_loss did not improve from 0.40104\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4155 - accuracy: 0.8531 - val_loss: 0.5053 - val_accuracy: 0.8242\n",
      "Epoch 241/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4016 - accuracy: 0.8579\n",
      "Epoch 241: val_loss improved from 0.40104 to 0.39135, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4026 - accuracy: 0.8581 - val_loss: 0.3914 - val_accuracy: 0.8706\n",
      "Epoch 242/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.8497\n",
      "Epoch 242: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4244 - accuracy: 0.8497 - val_loss: 0.5483 - val_accuracy: 0.7997\n",
      "Epoch 243/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8542\n",
      "Epoch 243: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4205 - accuracy: 0.8540 - val_loss: 0.4316 - val_accuracy: 0.8510\n",
      "Epoch 244/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.8533\n",
      "Epoch 244: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4194 - accuracy: 0.8533 - val_loss: 0.4457 - val_accuracy: 0.8406\n",
      "Epoch 245/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8605\n",
      "Epoch 245: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4142 - accuracy: 0.8606 - val_loss: 0.4982 - val_accuracy: 0.8221\n",
      "Epoch 246/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8519\n",
      "Epoch 246: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4195 - accuracy: 0.8519 - val_loss: 0.7885 - val_accuracy: 0.7102\n",
      "Epoch 247/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.8495\n",
      "Epoch 247: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4315 - accuracy: 0.8495 - val_loss: 0.4223 - val_accuracy: 0.8608\n",
      "Epoch 248/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8588\n",
      "Epoch 248: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4124 - accuracy: 0.8588 - val_loss: 0.3971 - val_accuracy: 0.8679\n",
      "Epoch 249/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4218 - accuracy: 0.8545\n",
      "Epoch 249: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4220 - accuracy: 0.8561 - val_loss: 0.4534 - val_accuracy: 0.8362\n",
      "Epoch 250/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8448\n",
      "Epoch 250: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4191 - accuracy: 0.8448 - val_loss: 0.5261 - val_accuracy: 0.8073\n",
      "Epoch 251/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4101 - accuracy: 0.8603\n",
      "Epoch 251: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4074 - accuracy: 0.8617 - val_loss: 0.4848 - val_accuracy: 0.8237\n",
      "Epoch 252/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.8466\n",
      "Epoch 252: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4392 - accuracy: 0.8466 - val_loss: 0.4923 - val_accuracy: 0.8215\n",
      "Epoch 253/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4045 - accuracy: 0.8599\n",
      "Epoch 253: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4044 - accuracy: 0.8597 - val_loss: 0.4558 - val_accuracy: 0.8466\n",
      "Epoch 254/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 0.8602\n",
      "Epoch 254: val_loss did not improve from 0.39135\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4044 - accuracy: 0.8602 - val_loss: 0.4779 - val_accuracy: 0.8242\n",
      "Epoch 255/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4078 - accuracy: 0.8570\n",
      "Epoch 255: val_loss improved from 0.39135 to 0.37794, saving model to models/classifier.hdf5\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4078 - accuracy: 0.8570 - val_loss: 0.3779 - val_accuracy: 0.8788\n",
      "Epoch 256/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.8637\n",
      "Epoch 256: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3924 - accuracy: 0.8637 - val_loss: 0.4567 - val_accuracy: 0.8384\n",
      "Epoch 257/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3985 - accuracy: 0.8634\n",
      "Epoch 257: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.3987 - accuracy: 0.8639 - val_loss: 0.4132 - val_accuracy: 0.8570\n",
      "Epoch 258/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.4014 - accuracy: 0.8685\n",
      "Epoch 258: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3996 - accuracy: 0.8682 - val_loss: 0.4421 - val_accuracy: 0.8455\n",
      "Epoch 259/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3902 - accuracy: 0.8664\n",
      "Epoch 259: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3916 - accuracy: 0.8657 - val_loss: 0.4643 - val_accuracy: 0.8297\n",
      "Epoch 260/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3901 - accuracy: 0.8643\n",
      "Epoch 260: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3944 - accuracy: 0.8641 - val_loss: 0.4773 - val_accuracy: 0.8308\n",
      "Epoch 261/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4037 - accuracy: 0.8611\n",
      "Epoch 261: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4054 - accuracy: 0.8613 - val_loss: 0.4570 - val_accuracy: 0.8368\n",
      "Epoch 262/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.3974 - accuracy: 0.8652\n",
      "Epoch 262: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3987 - accuracy: 0.8648 - val_loss: 0.4247 - val_accuracy: 0.8488\n",
      "Epoch 263/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4025 - accuracy: 0.8636\n",
      "Epoch 263: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4014 - accuracy: 0.8642 - val_loss: 0.3816 - val_accuracy: 0.8712\n",
      "Epoch 264/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.3997 - accuracy: 0.8601\n",
      "Epoch 264: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3988 - accuracy: 0.8601 - val_loss: 0.4824 - val_accuracy: 0.8264\n",
      "Epoch 265/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8665\n",
      "Epoch 265: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3869 - accuracy: 0.8664 - val_loss: 0.4495 - val_accuracy: 0.8346\n",
      "Epoch 266/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3954 - accuracy: 0.8641\n",
      "Epoch 266: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3946 - accuracy: 0.8650 - val_loss: 0.5056 - val_accuracy: 0.8248\n",
      "Epoch 267/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.4040 - accuracy: 0.8626\n",
      "Epoch 267: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4004 - accuracy: 0.8639 - val_loss: 0.4446 - val_accuracy: 0.8379\n",
      "Epoch 268/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3852 - accuracy: 0.8679\n",
      "Epoch 268: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3852 - accuracy: 0.8679 - val_loss: 0.4880 - val_accuracy: 0.8281\n",
      "Epoch 269/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8606\n",
      "Epoch 269: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4102 - accuracy: 0.8606 - val_loss: 0.3993 - val_accuracy: 0.8668\n",
      "Epoch 270/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8634\n",
      "Epoch 270: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3927 - accuracy: 0.8635 - val_loss: 0.4157 - val_accuracy: 0.8472\n",
      "Epoch 271/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8706\n",
      "Epoch 271: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3970 - accuracy: 0.8706 - val_loss: 0.5958 - val_accuracy: 0.7920\n",
      "Epoch 272/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3835 - accuracy: 0.8666\n",
      "Epoch 272: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3857 - accuracy: 0.8670 - val_loss: 0.5119 - val_accuracy: 0.8215\n",
      "Epoch 273/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8679\n",
      "Epoch 273: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3905 - accuracy: 0.8681 - val_loss: 0.4098 - val_accuracy: 0.8455\n",
      "Epoch 274/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8559\n",
      "Epoch 274: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4111 - accuracy: 0.8557 - val_loss: 0.4640 - val_accuracy: 0.8330\n",
      "Epoch 275/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8630\n",
      "Epoch 275: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3869 - accuracy: 0.8630 - val_loss: 0.4987 - val_accuracy: 0.8237\n",
      "Epoch 276/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8722\n",
      "Epoch 276: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3811 - accuracy: 0.8715 - val_loss: 0.4040 - val_accuracy: 0.8624\n",
      "Epoch 277/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3883 - accuracy: 0.8638\n",
      "Epoch 277: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3892 - accuracy: 0.8648 - val_loss: 0.4007 - val_accuracy: 0.8679\n",
      "Epoch 278/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.3799 - accuracy: 0.8678\n",
      "Epoch 278: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3806 - accuracy: 0.8672 - val_loss: 0.4482 - val_accuracy: 0.8439\n",
      "Epoch 279/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.8697\n",
      "Epoch 279: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3809 - accuracy: 0.8697 - val_loss: 0.4563 - val_accuracy: 0.8341\n",
      "Epoch 280/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3912 - accuracy: 0.8643\n",
      "Epoch 280: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3903 - accuracy: 0.8648 - val_loss: 0.4396 - val_accuracy: 0.8357\n",
      "Epoch 281/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3897 - accuracy: 0.8695\n",
      "Epoch 281: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3897 - accuracy: 0.8695 - val_loss: 0.4267 - val_accuracy: 0.8428\n",
      "Epoch 282/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.8715\n",
      "Epoch 282: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3806 - accuracy: 0.8721 - val_loss: 0.5848 - val_accuracy: 0.7980\n",
      "Epoch 283/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3840 - accuracy: 0.8723\n",
      "Epoch 283: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3831 - accuracy: 0.8721 - val_loss: 0.6521 - val_accuracy: 0.7833\n",
      "Epoch 284/1000\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3895 - accuracy: 0.8620\n",
      "Epoch 284: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3868 - accuracy: 0.8631 - val_loss: 0.4635 - val_accuracy: 0.8379\n",
      "Epoch 285/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8733\n",
      "Epoch 285: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3865 - accuracy: 0.8733 - val_loss: 0.4389 - val_accuracy: 0.8412\n",
      "Epoch 286/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8706\n",
      "Epoch 286: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3718 - accuracy: 0.8708 - val_loss: 0.3957 - val_accuracy: 0.8619\n",
      "Epoch 287/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8645\n",
      "Epoch 287: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3847 - accuracy: 0.8646 - val_loss: 0.3998 - val_accuracy: 0.8619\n",
      "Epoch 288/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.3740 - accuracy: 0.8737\n",
      "Epoch 288: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.3733 - accuracy: 0.8730 - val_loss: 0.4398 - val_accuracy: 0.8417\n",
      "Epoch 289/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.8625\n",
      "Epoch 289: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3860 - accuracy: 0.8628 - val_loss: 0.3900 - val_accuracy: 0.8739\n",
      "Epoch 290/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8675\n",
      "Epoch 290: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3720 - accuracy: 0.8668 - val_loss: 0.4301 - val_accuracy: 0.8559\n",
      "Epoch 291/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8691\n",
      "Epoch 291: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.3780 - accuracy: 0.8684 - val_loss: 0.5788 - val_accuracy: 0.8013\n",
      "Epoch 292/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8688\n",
      "Epoch 292: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3896 - accuracy: 0.8688 - val_loss: 0.4510 - val_accuracy: 0.8395\n",
      "Epoch 293/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3717 - accuracy: 0.8735\n",
      "Epoch 293: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3717 - accuracy: 0.8735 - val_loss: 0.3819 - val_accuracy: 0.8684\n",
      "Epoch 294/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3693 - accuracy: 0.8681\n",
      "Epoch 294: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.3693 - accuracy: 0.8681 - val_loss: 0.4311 - val_accuracy: 0.8444\n",
      "Epoch 295/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.3705 - accuracy: 0.8774\n",
      "Epoch 295: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3694 - accuracy: 0.8786 - val_loss: 0.4402 - val_accuracy: 0.8406\n",
      "Epoch 296/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8662\n",
      "Epoch 296: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3825 - accuracy: 0.8662 - val_loss: 0.5561 - val_accuracy: 0.8090\n",
      "Epoch 297/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3847 - accuracy: 0.8655\n",
      "Epoch 297: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3847 - accuracy: 0.8655 - val_loss: 0.4142 - val_accuracy: 0.8553\n",
      "Epoch 298/1000\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.3676 - accuracy: 0.8737\n",
      "Epoch 298: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3672 - accuracy: 0.8741 - val_loss: 0.4968 - val_accuracy: 0.8215\n",
      "Epoch 299/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8784\n",
      "Epoch 299: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3671 - accuracy: 0.8784 - val_loss: 0.5571 - val_accuracy: 0.8079\n",
      "Epoch 300/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3741 - accuracy: 0.8706\n",
      "Epoch 300: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.3741 - accuracy: 0.8706 - val_loss: 0.4000 - val_accuracy: 0.8624\n",
      "Epoch 301/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.3599 - accuracy: 0.8780\n",
      "Epoch 301: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3651 - accuracy: 0.8757 - val_loss: 0.4848 - val_accuracy: 0.8346\n",
      "Epoch 302/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.3714 - accuracy: 0.8696\n",
      "Epoch 302: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3710 - accuracy: 0.8706 - val_loss: 0.6385 - val_accuracy: 0.7680\n",
      "Epoch 303/1000\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3684 - accuracy: 0.8767\n",
      "Epoch 303: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3676 - accuracy: 0.8764 - val_loss: 0.4816 - val_accuracy: 0.8291\n",
      "Epoch 304/1000\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.8784\n",
      "Epoch 304: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.3591 - accuracy: 0.8784 - val_loss: 0.4852 - val_accuracy: 0.8346\n",
      "Epoch 305/1000\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8823\n",
      "Epoch 305: val_loss did not improve from 0.37794\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 0.3548 - accuracy: 0.8813 - val_loss: 0.4745 - val_accuracy: 0.8319\n",
      "Epoch 305: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x292ad3280>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/29 [==================>...........] - ETA: 0s - loss: 0.3898 - accuracy: 0.8734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 17:08:34.566707: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 7ms/step - loss: 0.3779 - accuracy: 0.8788\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_save_path)\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 17:08:39.339703: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 336ms/step\n",
      "[2.3585158e-05 1.8602614e-04 1.0290545e-05 4.2063023e-10 2.6251939e-03\n",
      " 9.2894050e-05 7.7759916e-01 2.6289831e-04 1.4346718e-03 2.1767288e-01\n",
      " 9.2371971e-05]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAGaCAYAAACPEgMrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABi4klEQVR4nO3de5xN9f7H8ddnxrgrRJihEJVuKJeUChUR0Y066aJ+6aJON3VOpVOdc3Q5J12cbocKKYUkFbkcl5Byy31cImIMoShUzOXz+2OtGduY2TNjr7X2nj2fp8d+mL322uu9vnv27O/+rvX9fpeoKsYYY4zJX0K0d8AYY4yJZVZRGmOMMWFYRWmMMcaEYRWlMcYYE4ZVlMYYY0wYVlEaY4wxYZSJ9g6URH+rf0OgY2qeSZ8VZJwxphTLPLhVvNxexq7vI/q8TKrRMOz+iEh5YDZQDqdO+0hVnxSR4cBFwC/uqreo6lIREeAVoAvwm7v823AZVlEaY4wpyQ4AHVR1n4gkAXNF5Av3sYdV9aM863cGGru31sAb7v8FsorSGGOMf7KzfN28OrPm7HPvJrm3cK3Y7sC77vO+EZGqIlJHVbcV9AQ7R2mMMcY/mh3ZrQhEJFFElgI7gGmqOt99aKCILBeRl0SknLssBdgS8vQ0d1mBrKI0xhjjn+zsiG4i0ldEFoXc+uaNUNUsVW0G1AVaicgZwKPAqUBLoDrwl6Mtgh16NcYY4xstYquw4OfrEGBIEdfdIyIzgctU9QV38QERGQb0d+9vBeqFPK2uu6xA1qI0xhhTYolITRGp6v5cAbgUWCMiddxlAvQAVrpP+RS4SRznAr+EOz8J1qI0xhjjp+zIWpRFUAcYISKJOI2/Mar6uYjMEJGagABLgTvd9SfhDA1ZjzM8pE9hAVZRGmOM8U+Eh14L3bzqcqB5Pss7FLC+Av2Kk2EVpTHGGP/4PDwkCKX6HKWI1BeRlYWvWbge/7qdRxa9Tr8pz+UuO71LK+6Z+jxPfT+S5DMb5C5PadqQuyY9w12TnuHuL56hSacWXuxCrqFDBpGetoylS6Z7ut1wOnVsx6qVs1mTOpdHHi7Wl7USkxnvedHIjPcylpa/xbACGB7it1JdUXppyUdzGHnzvw5b9uPaND6482V+WLDmsOU71qbx324DeKPLY7x707/oNvBWEhK9+1W8++4YLu96g2fbK0xCQgKDXxlI1269ObNpe3r16kGTJo3jKjPe86KRWRrKWBr+FkuDUlVRisiDIrLSvd2f57GGIrJERFoezbZ/WLCG33/Zd9iyXRvS+en7IztTZfxxkOws55tSmXJJ4eeQOApz5s7n5917vN1oGK1aNmfDhk1s3LiZjIwMxoyZwBXdOsVVZrznRSOzNJSxNPwtFirCcZSxoNRUlCJyDk7vptbAucDtQDX3sVOAcTiT4y4MYn/qNjuJe6Y+T78pz/HZgHdyK86SKDmlNlvS0nPvp23dRnJy7bjKjPe8aGSWhjIGLRbLp5od0S0WlKbOPG2B8aq6H0BEPgYuAGoCE4CrVDU1qJ1JW7qBVzv+hRonJXPVoDv5btYyMg9kBBVvjDHBiJFWYSRKTYsyjF+AzTgVaYFCp1H6du96z8J3bUjn4G9/cPzJdT3bZtDSt26nXt3k3Pt1U+qQnr49rjLjPS8amaWhjEGL9/JFS2mqKOcAPUSkoohUAq50lx10f75JRP5U0JNVdYiqtlDVFmdXaRTRjlStWzO3886xKTWocVIye9J2RrTNaFq4aCmNGjWgfv16JCUl0bNndz77fGpcZcZ7XjQyS0MZgxaT5YuDXq+l5tCrqn7rXshzgbvoLWC3+9h+EekKTBORfar6aXG3f83gfjQ4twkVq1Xhoa//w8yXPuL3X/bT5ambqVS9Cr3feZjtq3/g3Zue58SWp3DBXd3IysxCs7P5/Ilh/LZ7X+EhRfTeyNe46MI21KhRnU3fL+Lpv7/AsOEferb9vLKysrjv/gFMmjiKxIQEho8YTWrqOt/yopEZ73nRyCwNZSwNf4uFioNxlOJMUmCK42/1bwj0RXsmfVaQccaYUizz4FbxcnsHVs+M6POyXJP2nu7P0Sg1LUpjjDFRYJ15jDHGmPhmLUpjjDH+iZEOOZGwitIYY4x/4uDQq1WUxhhjfKNa8nu9WkVpjDHGP3Fw6NU68xhjjDFhWIvSGGOMf+wcpTHGGBNGHBx6tYrSGGOMf+JgCjs7R2mMMcaEYS3KoxD03Ksv12ofaN79P84MNK+0CHrCSpvF2cQEO/RqjDHGhGGdeYwxxpgwrEVpjDHGhBEHLUrrzGOMMcaEYS1KY4wx/omDFqVVlMYYY3xjk6IbY4wx4ViL0hhjjAkjDnq9WmceY4wxJowSUVGKSLKIfFTIOveLSMWg9qk4OnVsx6qVs1mTOpdHHu7n2XY7vHA7ty55jev/92zustb9r+G6qc/Qa/JArnj/L1SqVTX3sQuevpHecwZx3dRnqHlGfc/2A/wrYyxlBplXt24y06aOZdmymSxdOoN777nN17wc8fyaRisz3vMKlZ0d2S0GiGp8THQlIpuAFqq6qxjPSdSjONNcpmxKkV+0hIQEVq+aw2VdrictbRvffD2J3jfezerV3xU5r6Ap7JJbn0LG/gNc8vIdfHDJowAkVa5Axr7fATirT0eqN05h1mPDOLF9U87q05HPbvo3tZqfxAVP38hHVzyV73aLO4WdF2UsrqAzvcgrzhR2tWsfT53ax7Nk6UoqV67E/PmTueaaW4uVV9y/7JL4msZ6ZknMyzy41dPZFn//35sRVTIVLrkz6NkfjxBzLUoReU5E+oXcf0pE+ovISvd+ooi8ICIrRWS5iNwrIn8GkoGZIjLTXe96EVnhrvd8yPb2icggEVkGPC4in4Q8dqmIjPeyPK1aNmfDhk1s3LiZjIwMxoyZwBXdOnmy7fT5a/ljz77DluVUkgBJFcuh7sdlg47nsGbcXAB+XLKBcsdUouLxVT3ZDz/LGCuZQedt376DJUtXArBv337WrPmO5OTavuVB/L+m0ciM97wiiYMWZcxVlMBooGfI/Z7A/JD7fYH6QDNVPQt4X1UHA+lAe1VtLyLJwPNAB6AZ0FJEerjPrwTMV9WmwD+AU0WkpvtYH+AdLwuTnFKbLWnpuffTtm7z/QPv3Eeu5eb5r3Dylecx/4VxAFSuXY196T/lrrNv289Url3Nk7xolDHozGiUMceJJ9alWdMzWLBgia85peE1jfcyRvN9Gs9irqJU1SXA8e55yabAbmBLyCqXAP9V1Ux3/Z/z2UxLYJaq7nTXex+40H0sCxjnPleBkUBvEakKtAG+8L5UwfrmX2MZ0fo+1o2fx1m3XBrt3TERqFSpImNGD+Wh/k+yd+++wp9gTKzR7MhuMSDmKkrXWOAaoBdOC9NLf+Q5LzkM6A1cD4zNqYDzEpG+IrJIRBZlZ+8vclj61u3Uq5uce79uSh3S07cf3Z4X09rx8zipS0sA9m3fTeXk43Ifq1ynOvu27/YkJxplDDozGmUsU6YMY0YP5YMPxvPJJ/5/fysNr2m8lzGanzcFskOvvhkNXIdTWY7N89g04A4RKQMgItXd5XuBKu7PC4CLRKSGiCTiVIJf5hekquk4h20H4FSa+VLVIaraQlVbJCRUKnJBFi5aSqNGDahfvx5JSUn07Nmdzz6fWuTnF9ex9Wvl/tyw49nsXr8NgI3TvuXUq9sCUKv5SRzc+xu/7djjSWbQZYxGZjTKOHTIINasWc/LrwzxNSdHaXhN472M0XhNC+VzRSki5UVkgYgsE5FVIvK0u7yBiMwXkfUiMlpEyrrLy7n317uP1y8sIyYnHFDVVSJSBdiqqtvyFOQt4GRguYhkAEOBV4EhwGQRSXfPU/4VmInT2XCiqk4IE/k+UFNVV3tdlqysLO67fwCTJo4iMSGB4SNGk5q6zpNtd3y1HynnNqF89crcsmAw8weNo36HplQ9qQ6arexN28Wsx5y6/4cZSzmxQ1NunDuIzN8PMv0h7z58/SxjrGQGnXf+eS3p3fsaVqxIZdFC54NuwBPPMXnyDN8y4/01jUZmvOcVif+HTw8AHVR1n4gkAXNF5AvgQeAlVf1QRN4EbgPecP/fraqNROQ6nP4svcIFxM3wkEiIyKvAElV9uyjrF2d4iBcKGh7il+IODzFFE3Qfd/vLNkfD8+Ehn78Y2fCQrg8WeX/csfRzgbuAiUBtVc0UkTbAU6raSUSmuD9/7R6Z3I7TUCpwP2P10GtgRGQxcBbwXrT3xRhj4k4A5yjdYYNLgR04p+c2AHtC+pykASnuzym4HUTdx38BjiOMmDz0GiRVPSfa+2CMMXErwkOvItIXZ1hgjiGqeti5I7eDZjN39MJ44NSIQvMo9RWlMcYYH0XYc9WtFIvUqUJV97iTzrQBqopIGbfVWBfY6q62FagHpLmHXo8Ffsp3g65Sf+jVGGNMySUiNd2WJCJSAbgUWI3TmfMad7WbgZwOnZ+693EfnxHu/CRYi9IYY4yf/O/1WgcY4Q4FTADGqOrnIpIKfCgi/wSWADmdNd8GRorIeuBnnKGIYVlFaYwxxj8+TxqgqsuB5vks/x5olc/yP4Bri5NhFaUxxhj/xMjsOpGwitIYY4x/4mCsvnXmMcYYY8KwFqUxxhj/2KFXY4wxJgyrKEunpMRgX7ag5159sk67QPMAnt42K/DMoJX8MzXhJUjQs9lC0HNVx/vv0Bcxck3JSFhFaYwxxj9x0KK0zjzGGGNMGNaiNMYY4584GB5iFaUxxhj/xMGhV6sojTHG+CcOKko7R2mMMcaEYS1KY4wx/rHhIcYYY0zBNNs68xhjjDEFs3OUJZ+I3CIir7o/PyUi/b3OuPfe21i8eBqLFk1lxIjBlCtXzuuII3Tq2I5VK2ezJnUujzzcz5Ntdvv37Ty4+HXumPpc7rImXVpx57TnGbBxJHXObJC7/Iwe53H7pGdybwM2jqTWaSd6sh85/Chjac6LVmZCQgIL5k9m/PjhvmfVrZvMtKljWbZsJkuXzuDee27zNW/okEGkpy1j6ZLpvuaEisbvMCzNjuwWA0p9Rem35ORa3H13H84/vystWnQkMTGRa6/t5mtmQkICg18ZSNduvTmzaXt69epBkyaNI97usrFzGHXzvw5btnNdGmPveJkf5q85bPnKT+YxtMtjDO3yGBMeeIPdW3byY+oPEe9DDr/KWFrzopUJzhfJNWvW+54DkJmZySOPPE3Tpu1p27Ybd951i69lfPfdMVze9Qbftp9XtH6H8S5uK0oRuUlElovIMhEZKSLdRGS+iCwRkf+JSK2g9qVMmUQqVChPYmIiFSpUYNu2H33Na9WyORs2bGLjxs1kZGQwZswErujWKeLtbl6wht/37Dts2a716fz0/bawzzv9ijakfvZ1xPmh/Cpjac2LVmZKSh06d76Yd4aN8jUnx/btO1iydCUA+/btZ82a70hOru1b3py58/l59x7ftp9XNH6HhcrWyG4xIC4rShE5HRgAdFDVpsB9wFzgXFVtDnwIPBLEvqSn/8jLLw9h3bqv2bhxIb/+upfp0+f4mpmcUpstaem599O2bvP1w6Awp3U7l5UTvK0ogy5jvOdFK3PQC0/x6KMDyY7CB+KJJ9alWdMzWLBgSeDZfom1v33AOUcZyS0GxGVFCXQAxqrqLgBV/RmoC0wRkRXAw8DpxdmgiPQVkUUisigzc1/hT3BVrXoMXbt2pEmTtjRs2IpKlSpw3XVXFie6REtudhKZvx9k57q0aO+KiTFdulzMjp27WLJkReDZlSpVZMzooTzU/0n27i3637M5ClZRlij/AV5V1TOBO4DyxXmyqg5R1Raq2qJMmcpFfl6HDm3ZtGkLu3b9TGZmJp98Mplzzz2neHteTOlbt1OvbnLu/bopdUhP3+5rZkFO79aGlZ/O83y7QZcx3vOikXlem5Z0vbwj69Z+zXsjX6N9u/MZPmywb3k5ypQpw5jRQ/ngg/F88skXvucFKZb+9nOpRnaLAfFaUc4ArhWR4wBEpDpwLLDVffzmoHZky5Z0WrVqToUKTr3cvv35rF3rb8eFhYuW0qhRA+rXr0dSUhI9e3bns8+n+pqZLxFO69qaVZ96e9gVgi9jvOdFI3PAE8/R8KSWnHxKG3rf2I+Zs77ilj5/9i0vx9Ahg1izZj0vvzLE96ygxczffpyJy3GUqrpKRAYCX4pIFrAEeAoYKyK7cSrSBmE24ZmFC5cyfvwkvv56IpmZWSxbtoq33/a340JWVhb33T+ASRNHkZiQwPARo0lNXRfxdq8c3I8T2zShYrUq3PfNf/jypY/4fc9+Lnv6ZipWr8J1wx7mx9QfGHXT8wCc2PpUfk3/mT1bdkacnZdfZSytedHKDNr557Wkd+9rWLEilUULnQpkwBPPMXnyDF/y3hv5Ghdd2IYaNaqz6ftFPP33Fxg2/ENfsiBGf4cxcvg0EhL0FcLjQYUKJwb6omVkZQYZx5N12gWaB/D0tlmBZxpvJYgEnhn051dp+LTMPLjV01/kby/8X0QvW8X+bwX/xsojLluUxhhjYkSMTBoQiXg9R2mMMcZ4wlqUxhhj/BMjkwZEwipKY4wxvtE46MxjFaUxxhj/WIvSGGOMCcM68xhjjDHxzVqUxhhj/GOHXo0xxpgwrDOPMcYYE4a1KEunoKeUC1o0ppObW6N1oHltd80PNK80yLbpME1+rDOPMcYYE9+sRWmMMcY/cXDo1VqUxhhjfKPZ2RHdCiMi9URkpoikisgqEbnPXf6UiGwVkaXurUvIcx4VkfUislZEOhWWYS1KY4wx/vG/RZkJPKSq34pIFWCxiExzH3tJVV8IXVlETgOuA04HkoH/icjJqppVUIC1KI0xxpRYqrpNVb91f94LrAZSwjylO/Chqh5Q1Y3AeqBVuAyrKI0xxvgnWyO7FYOI1AeaAznd2u8RkeUi8o6IVHOXpQBbQp6WRviK1SpKY4wxPtLsiG4i0ldEFoXc+uYXIyKVgXHA/ar6K/AGcBLQDNgGDDraItg5SmOMMf6J8Bylqg4BhoRbR0SScCrJ91X1Y/d5P4Y8PhT43L27FagX8vS67rICWYvSGGOMbzRbI7oVRkQEeBtYraovhiyvE7LalcBK9+dPgetEpJyINAAaAwvCZVhFGYBOHduxauVs1qTO5ZGH+8Vlpl95DV/sxznLh3HWjJdzlzV+8yHOnDaIM6cNovn8NzlzmnNEpVKzRrnLz5z2ItUu83a2n3h5TWMp08pY8vNiwPnAjUCHPENB/iUiK0RkOdAeeABAVVcBY4BUYDLQL1yPVwDRKE475Z54/VxVzwg49y3gRVVNPZrnlymbUuQXLSEhgdWr5nBZl+tJS9vGN19PoveNd7N69XdHEx2TmV7kFTSFXZXWp5H12x80euXPLO9w/xGPn/C3W8jau5+tL40loUJZsg9mQlY2ScdX46z/vcji5rdB1pFjsYo7hV1JfE1jPdPKGJt5mQe3ipf7tPfPXSOqZKoM/tzT/TkapbJFqar/d7SVZHG1atmcDRs2sXHjZjIyMhgzZgJXdCt0fGuJyvQzb+/8VLJ27y3w8eOuOI+fPpkLQPbvB3MrxYRySXj5JTCeXtNYybQylvy8IsnOjuwWA2KhokwUkaHujApTRaSCiNwuIgtFZJmIjBORigAiMlxEBovIPBH5XkSucZcniMjrIrJGRKaJyKScx/IjIrNEpIX78z4RGehmfSMitbwsXHJKbbakpefeT9u6jeTk2l5GRD0zGmUEp7WZsXMPf2zclruscvPGnDXzZc6a8RIb//LffFuTR6M0vKZWRsvzRYDDQ/wSCxVlY+A1VT0d2ANcDXysqi1VtSnO4NHbQtavA7QFugLPucuuAuoDp+Ecq25TjPxKwDdu1mzg9qMuiQlUjR5tc1uTOfYt+Y7l7e9nRedHSLn3KqRcUpT2zhgDWEXpkY2qutT9eTFOhXeGiMwRkRXADThTDeX4RFWz3UOnOa2/tsBYd/l2YGYx8g9yqNtwTv4RQsfyZGfvL/LG07dup17d5Nz7dVPqkJ6+vRi7V3xBZ0ajjCQmUK3Lufz06Vf5PvzH+q1k7f+Diqec4ElcaXhNrYyWZ/IXCxXlgZCfs3DGdg4H7lHVM4GngfIFrO/FSd4MPXQyKyf/CKo6RFVbqGqLhIRKRd74wkVLadSoAfXr1yMpKYmePbvz2edTPdjt2MmMRhmPvaApf6zfysFtP+UuK1fveEh03tJlU2pSoVEKB9J2eJJXGl5TK6Pl+UFVI7rFglidcKAKsM0dRHoDhQwGBb4CbhaREUBNoB0wytc9LKKsrCzuu38AkyaOIjEhgeEjRpOaui6uMv3Ma/T6AxzT5gzKVK9C80VDSRv0ITs/mE6N7uez65M5h61bpVUTTrnnSjQzC7KVjY8NIfPngjsCFUc8vaaxkmllLPl5RRIjh08jEVPDQ0SkP1AZ+BF4BNiJM2dfFVW9RUSGu+t/5K6/T1Uri0gC8DpOBbkFp6X5vKpOIx8iMgvor6qLcrbhLr8G6Kqqt4Tb7+IMDzFFU9DwEL8Ud3iIMaWF18NDfr3t0og+L495e1rUh4dEtaL0kohUVtV9InIcziwL57vnKz1nFaX3rKI0JjZYRXmkWD30ejQ+F5GqQFngH35VksYYY4quKNPQxbq4qShVtV3eZSIyHmiQZ/FfVHVKIDtljDGlnVWUsU1Vr4z2PhhjTKkWG5PrRCSuK0pjjDHRFQ+HXmNhHKUxxhgTs6xFaYwxxj9x0KK0itIYY4x/7BylMcYYU7B4OEdpFaUxxhj/xEGL0jrzGGOMMWFYi7IEqFS2fOEreWj/wT8CzYPgp5S7tNZZgeYBfLP7u0Dz9mcE+3vMjpPpMMNJSgz+IzMjKzPwTC/ZoVdjjDEmnDg49GoVpTHGGN9oHFSUdo7SGGOMCcNalMYYY/wTBy1KqyiNMcb4Jh4OvVpFaYwxxj9WURpjjDEFi4cWpXXmMcYYY8KwFqUxxhjfWIvSFEmnju1YtXI2a1Ln8sjD/XzJePX151i/cQFfL/gid9njTzzAV99MZM68zxg/YTi1ax/vSzYEU8ZoZFY6phKPv/k4Q2YO4b8z/supZ5/KX1//K69OfpVXJ7/K8HnDeXXyq57l/ef1Z1m3cT7zFkw64rF+997G7n3rqX5cNc/y8pOQkMCC+ZMZP364rzkQv++bUPfeexuLF09j0aKpjBgxmHLlyvmaF43XNBzNjuwWC6yi9FlCQgKDXxlI1269ObNpe3r16kGTJo09zxn1/jiu7tHnsGWDXx7K+edezgXndWPy5Jn85dF7Pc+F4MoYjcw7n7qTRbMW0bd9X/p16seW9Vt47u7nuOeye7jnsnuY+8Vc5n0xz7O8D97/mGt63HrE8pSUOrS/uC1bNm/1LKsg9957G2vWrPc9J57fNzmSk2tx9919OP/8rrRo0ZHExESuvbabb3nReE0LpRLZLQbEVEUpIg+KyEr3dr+I1BeR1SIyVERWichUEangrnuSiEwWkcUiMkdETg2z3eEick3I/X3u/+1EZLaITBSRtSLypoh4+pq0atmcDRs2sXHjZjIyMhgzZgJXdOvkZQQA875ayO7dew5btnfvvtyfK1WsgPo0F2dQZQw6s2KVipzR+gymfDgFgMyMTPb/uv+wdS7seiGzJszyLDO/3yPAwOcf56kBz/v2O8yRklKHzp0v5p1ho3zNgfh93+RVpkwiFSqUJzExkQoVKrBt24++ZUWjfIWxFqWHROQcoA/QGjgXuB2oBjQGXlPV04E9wNXuU4YA96rqOUB/4PWjjG4F3AucBpwEXHWU28lXckpttqSl595P27qN5OTaXkaE9cSTD7FqzVyu7dWdgf982ZeMaJQxiMza9Wrzy8+/8OCLD/LqF69y37/uo1yFQ4fNzmh9Brt37SZ9U3qYrUSu8+WXsC19OytXrvE1B2DQC0/x6KMDyQ5gIut4fd+ESk//kZdfHsK6dV+zceNCfv11L9Onz/EtL9qfN/EqZipKoC0wXlX3q+o+4GPgAmCjqi5111kM1BeRysB5wFgRWQr8F6hzlLkLVPV7Vc0CPnD3I2784+lBnH5qW8aOnkDfO26M9u6UKIllEml0RiMmvjuRezrfwx+//UHPfj1zH2/XvR1fTvjS132oUKE8D/a/k2d9+pITqkuXi9mxcxdLlqzwPau0qFr1GLp27UiTJm1p2LAVlSpV4Lrrroz2bgVKsyWiWyyIpYqyIAdCfs7C6ambAOxR1WYhtyZhtpHpPgf30GrZkMfyfnXO96u0iPQVkUUisig7e39+q+Qrfet26tVNzr1fN6UO6enbi/x8r4wZPYErul/my7ajUcYgMndt28WubbtYu3QtAHMnzaXRGY0ASEhM4LzLzmP2p7M9zcyrQcMTOLF+PeZ8/TnLVs0iOaU2X86dwPHH1/A867w2Lel6eUfWrf2a90a+Rvt25zN82GDPc3LE6/smVIcObdm0aQu7dv1MZmYmn3wymXPPPce3vFj5vAllh169NQfoISIVRaQScKW77Aiq+iuwUUSuBRBH0zDb3gTkvDuvAJJCHmslIg3cCrQXMLeAzCGq2kJVWyQkVCpyoRYuWkqjRg2oX78eSUlJ9OzZnc8+n1rk50ei4Un1c3/u0vVSvlu3wZecaJQxiMzdO3ezc9tOUhqmANDs/GZs/m4zAM0vaE7ahjR2bd/laWZeqavWcXKD1jQ9vR1NT29H+tbtXNS2Ozt2eJ874InnaHhSS04+pQ29b+zHzFlfcUufP3uekyNe3zehtmxJp1Wr5lSo4FxTtn3781m71r+OUtH8vCmIqkR0iwUxM45SVb8VkeHAAnfRW8DuME+5AXhDRAbgVHwfAssKWHcoMEFElgGTgdAm4ULgVaARMBMYf7RlyE9WVhb33T+ASRNHkZiQwPARo0lNXedlBABvD3uZthe05rjjqpG6di7PDnyFjp3a0ahxQ7Kzs9myeSsP3PeE57kQXBmjkfnGE2/wyH8eISkpiW2bt/HSQy8BcNEVF3naiSfHW8Ne4nz397hy7VyeG/gK77071vOcWBDP75scCxcuZfz4SXz99UQyM7NYtmwVb7/tX0epaLympYH43YsulolIO6C/qnYtzvPKlE0J9EWrVLZ8kHHsP/hHoHnRcGmtswLP/Gb3d4Hm7c8I9veYXQo+S5ISg29bZGRlBpqXeXCrp824tNYdInpj1J0/I+rNylg69GqMMSbO+N2ZR0TqichMEUl1hxHe5y6vLiLTROQ79/9q7nIRkcEisl5ElovI2YVlxFVFKSKPi8jSPLfHC1pfVWcVtzVpjDGm6FQjuxVBJvCQqp6GM7Swn4icBvwVmK6qjYHp7n2AzjjDDhsDfYE3CguImXOUXlDVgcDAaO+HMcYYh99DPFR1G7DN/XmviKwGUoDuQDt3tRHALOAv7vJ31Tnv+I2IVBWROu528hVXLUpjjDGll4jUB5oD84FaIZXfdqCW+3MKsCXkaWnusgLFVYvSGGNMbIm0RSkifXEOkeYYoqpD8lmvMjAOuF9VfxU5lKuqKiJH3anIKkpjjDG+ibQztFspHlExhhKRJJxK8n1V/dhd/GPOIVURqQPscJdvBeqFPL2uu6xAdujVGGOMbwLo9SrA28BqVX0x5KFPgZvdn28GJoQsv8nt/Xou8Eu485NgLUpjjDEl2/nAjcAKd+5vgMeA54AxInIb8AOQM1HzJKALsB74DediHGFZRWmMMcY3fk9Dp6pzgYJCLs5nfQWKdUVrqyiNMcb4JlYmNo+EVZTGGGN8kx0jE5tHwirKEqB8YlLhK3loP/E/1+v/flweeOZ/arUPNO+eH2cGmlcaBD3vajyIlSuARMJ6vRpjjDFhWIvSGGOMb/yewi4IVlEaY4zxTTxcfc0qSmOMMb6xFqUxxhgTRjz0erXOPMYYY0wY1qI0xhjjm3gYHmIVpTHGGN/EQ2eemDn06l5l+u5o74cxxhjvZKtEdIsFMVNRAlWBuKwoO3Vsx6qVs1mTOpdHHi7WXLxFlpxSm48/G8Hs+Z/z5TefcfudNwJQtdqxjPnkbb7+djJjPnmbY6se40t+EGWMZmbduslMmzqWZctmsnTpDO695zbPtt3uhdu5eclr9Pzfs7nLWva/hmunPsM1kwdy+ft/oWKtqgDU73h27vKrJv6d2i1P9mw/hg4ZRHraMpYume7ZNgsT9PumNJQxGn+L8U40RtrFIvIh0B1YCywBxqvqpyIyHtitqreKyK3ASar6uIg8CNzqPv0tVX25gO3WBz5X1TPc+/2Byqr6lIjMApYBF+Echr5VVRcUtq9lyqYU+UVLSEhg9ao5XNbletLStvHN15PofePdrF79XVE3wXEVqhS6zvG1alKrdk1WLEulUuVKTPtyHLf8qR+9briSPbt/4T8vDeXeB27n2KrH8M8nB4Xd1k+/7y3yvoE3ZSyuSDOL+z21du3jqVP7eJYsXUnlypWYP38y11xza7HKWNAUdnVan0LG/gN0ePkOxlzyKABJlSuQse93AM7o05FqjVOY89gwylQsR+ZvBwCofmo9Ln3jXka3fyTf7RZ3CrsL2rZm3779DBv2Cs2aH3HRBc9F430T72X0Ii/z4FZPm3FLTugeUSXTfPOEqDcrY6lF+Vdgg6o2A6YAF7jLU4DT3J8vAGaLyDk41xBrDZwL3C4izY8yt6KbeTfwzlFuo0CtWjZnw4ZNbNy4mYyMDMaMmcAV3Tp5HcOOH3eyYlkqAPv37ee7tRuonVyLy7pczOhRnwAwetQndL78Es+zgypjNDO3b9/BkqUrAdi3bz9r1nxHcnJtT7a9bf5aDuzZd9iynEoSIKliOcD5rMmpJHOXe/hFd87c+fy8e49n2ytMNN438V7GaLymhVGN7BYLYqmiDDUHuEBETgNSgR9FpA7QBpgHtMVpce5X1X3AxxyqWIvrAwBVnQ0cIyJVI935UMkptdmSlp57P23rNs8+YAtS74QUzjirCd8uWkbNmsex48edgFOZ1qx5nOd50ShjNDJznHhiXZo1PYMFC5b4mtPqkWvpPf8VGl95HgtfGJe7vP5lLeg18190HtGfWf2H+roPform7zAoQZcxFl9TO0fpE1XdinPO8jJgNk7F2RPYp6rFOy4ImRxezvJ54wq5D4CI9BWRRSKyKDt7fzF3ITgVK1Xk7ZGDeeLRZ9m398j91PyLZ4qoUqWKjBk9lIf6P8nevfsKf0IEFvxrLO+1vo/vxs/jjFsuzV2+afIiRrd/hCn/9xIt+1/j6z4YEylViegWC2KpotwLhJ6M+wa4n0MVZX/3f9z/e4hIRRGpBFwZ8lhePwLHi8hxIlIO6Jrn8V4AItIW+EVVf8lvI6o6RFVbqGqLhIRKRS5U+tbt1KubnHu/bkod0tO3F/n5xVGmTBneGTmYcWM+Y9Jn0wDYufMnjq9VE3DOY+7a+bPnuUGWMZqZZcqUYczooXzwwXg++eQLX7NCfTd+Hg27tDxi+bb5aznmhOMpX61yYPvipWj8DoMWdBlLw2saDTFTUarqT8BXIrJSRP6NU/GVUdX1wLdAdXcZqvotMBxYAMzH6cyT73EwVc0A/u6uOw1Yk2eVP0RkCfAm4F1XRtfCRUtp1KgB9evXIykpiZ49u/PZ51O9jgHgpVf/yXdrN/Df14bnLpvyxQx6/akHAL3+1IPJk7zv7RdkGaOZOXTIINasWc/LrwzxNQfg2Pq1cn+u3/Fsdq/fBsAxIctrnFGfxHJl+GO3vy1bv0Tjdxi0oMsYi69pPBx6jakJB1T1T3kWve0uzwAq5Vn3ReDFIm53MDC4gIffU9X7i7enRZeVlcV99w9g0sRRJCYkMHzEaFJT13me0+rcs+l5fQ9SV65l+pzxADzz95f4z4tDGTriJf5049WkbUnn9lse8Dw7qDJGM/P881rSu/c1rFiRyqKFzgfPgCeeY/LkGRFv++JX+5F8bhPKV69M7wWDWTRoHCd0aErVk+qg2cretF3MeWwYAA07t+Tkq9uSnZlF5h8HmXb3qxHn53hv5GtcdGEbatSozqbvF/H0319g2PAPPdt+XtF438R7GaPxmhYmHk72xMzwkGhwh4f0V9VFxXlecYaHeKEow0O8VNzhISVRNL6nFjQ8xC/FHR5iDHg/PGRenasj+rw8b9u4qDcrY6pFGQkROQ7I77jixe5h3SOoajtfd8oYY0q5WOmQE4m4qSjdyrBZtPfDGGNMfImbitIYY0zsyY72DnjAKkpjjDG+0aj0CPCWVZTGGGN8kx0H/UVjZhylMcYYE4usRWmMMcY32Xbo1RhjjCmYnaM0xhhjwrBer8YYY0wY1qI0gSgNU8oFLRod8YKeUm5EjWCnzLt5V/xPmVelbIXAM/ce/L3wlYyvrKI0xhjjGzv0aowxxoRhFaUxxhgThp2jNMYYY8LILvn1pM3MY4wxxoRjFaUxxhjfZCMR3QojIu+IyA4RWRmy7CkR2SoiS91bl5DHHhWR9SKyVkQ6FaUMVlEaY4zxjUZ4K4LhwGX5LH9JVZu5t0kAInIacB1wuvuc10UksbAAqyiNMcb4JjvCW2FUdTbwcxF3pzvwoaoeUNWNwHqgVWFPsorSGGNMPLpHRJa7h2aructSgC0h66S5y8KyijIAnTq2Y9XK2axJncsjD/eLy8x4L+PQIYNIT1vG0iXTfc0J5Vf5zn3xdq5e/hqXz3j2iMdOvaMzN6S/R7nqlQ9bXr1pQ67fPIJ6l7f0bD8gPt83/3n9WdZtnM+8BZOOeKzfvbexe996qh9XLZ9neiMar2k42SIR3USkr4gsCrn1LULsG8BJQDNgGzAokjIUWlGKSP3Qk6ReEZHhInJNMZ/TTkQ+93pfQrb/lIj093KbCQkJDH5lIF279ebMpu3p1asHTZo09jIi6pmloYzvvjuGy7ve4Nv28/KzfN+Pns2MG/59xPKKydWpc9GZ7E/bddhySRCaP96LbV+u8CQ/R7y+bz54/2Ou6XHrEctTUurQ/uK2bNm81dO8UNF4TQsT6TlKVR2iqi1CbkMKzVT9UVWzVDUbGMqhw6tbgXohq9Z1l4VlLUqftWrZnA0bNrFx42YyMjIYM2YCV3QrUkerEpNZGso4Z+58ft69x7ft5+Vn+XbMX8vB3fuOWH7OU71Z8s8PUT28C8XJt3Zky6SF/LHrV0/yc8Tr+2beVwvZnc97ZeDzj/PUgOePeH29FI3XtDB+n6PMj4jUCbl7JZDT2PsUuE5EyolIA6AxsKCw7RW1okwUkaEiskpEpopIBRG5XUQWisgyERknIhXdHRwuIoNFZJ6IfJ/TahTHq26X3P8BxxdS0JbuNpaJyAIRqZLn8eoi8ol7DPobETnLXX5RSJfgJTnPE5GH3f1dLiJPh2zncRFZJyJzgVOK+HoUWXJKbbakpefeT9u6jeTk2l7HRDWzNJQxaEGXr26ns/lt+272pG4+bHmF2tWo17kF60Z4f8i5NL1vOl9+CdvSt7Ny5Rpfc2Lx7yJbIrsVRkQ+AL4GThGRNBG5DfiXiKwQkeVAe+ABAFVdBYwBUoHJQD9VzSoso6gz8zQGrlfV20VkDHA18LGqDnV39J/AbcB/3PXrAG2BU3Fq8I9wavVTgNOAWu6OvlNAwcsCo4FeqrpQRI4B8k6h/zSwRFV7iEgH4F2c49H9cQr/lYhUBv4QkY5uGVoBAnwqIhcC+3G6CjdzX4tvgcVFfE2MiQuJFcpy+r1XMOP654947Jyne7Nk4IfgYyso3lWoUJ4H+9/J1d1vifauxCVVvT6fxW+HWX8gMLA4GUWtKDeq6lL358VAfeAMt4KsClQGpoSs/4l7bDhVRGq5yy4EPnBr73QRmREm7xRgm6ouBFDVXwFEDvt60RanwkZVZ4jIcW6F+hXwooi8j1OZp7kVZUdgifvcyjgVZxVgvKr+5m7/04J2yD2B3BdAEo8lIaFSmN0/JH3rdurVTc69XzelDunp24v03KMVdGZpKGPQgixflROPp/IJNenyv2cAqFinOp2n/JPJXZ7kuKYNaPvGPQCUq16FlIubolnZpE2O/PtkaXnfNGh4AifWr8ecr53uFckptfly7gQuvugqduzYVciziycW/y6KMmlArCvqodcDIT9n4VSww4F7VPVMnNZd+QLWD/RVUtXngP8DKgBficip7j48GzL4tJGqFviNo4Dt5p5QLmolCbBw0VIaNWpA/fr1SEpKomfP7nz2+dTiRBdb0JmloYxBC7J8e9akMe6sfkxo/QATWj/Ab9t+5otOA/hj5y9MOPfB3OWbP1/AgkeHe1JJQul536SuWsfJDVrT9PR2ND29Helbt3NR2+6eV5IQm38XAUw44LtIOvNUAbaJSBJQlO6As4FeIpLonmgNd1XZtUAdEWkJICJVRCRv63dOTq6ItAN2qeqvInKSqq5Q1eeBhTiHf6cAt7qHYhGRFBE53t2nHu451ypAtyKVvBiysrK47/4BTJo4ipXLZ/HRR5+RmrrO65ioZpaGMr438jXmzv6UU04+iU3fL6LPLdf5lgX+lu/81/vR6bOnOOakOly5aDAnXX+RJ9strnh937w17CWmzhhLo8YNWLl2Lr1vutbT7YcTjde0MH6fowyCFNYDS0TqA5+r6hnu/f44hy5/BB4BdgLzgSqqeouIDHfX/8hdf5+qVhbnuOl/gEuBzUAG8E7OevnktnTXr4BzfvISoAXQX1W7ikh1nHOcDYHfgL6qulxE/oNTCWcDq4BbVPWAiNyH09IE2Af0VtUNIvI4cDOww92vb1X1hXCvSZmyKbHyRceYAo2oEe67qPdu3jUz0LxoqFK2QuCZew/m7Z7hr8yDWz2tnoan9I7o8/KWre9FvbostKI0R7KK0pQEVlF6zyrK4ouHitKuR2mMMcY38dCqiHpFKSLjgQZ5Fv9FVafkt74xxpiSI1bOM0Yi6hWlql4Z7X0wxhjjj6OdXSeW2BR2xhhjTBhRb1EaY4yJX/HQorSK0hhjjG/UzlEaY4wxBbMWpTHGGBNGPFSU1pnHGGOMCcNalMYYY3xjEw4YY2JW0FPKvXF8sFPmAdy1I9gyBj2dXDywCQeMMcaYMOLhHKVVlMYYY3wTDxWldeYxxhhjwrAWpTHGGN9YZx5jjDEmDOvMY4wxxoRh5yiNMcaYOGctSmOMMb6xc5TGGGNMGNlxUFVGdOhVROqLyEqvdiZku8NF5BqvtxstnTq2Y9XK2axJncsjD/eLy0wrY8nP8yvzwhdup/fS17j6f8/mLjun/zVcNe0ZrpoykM7v/4WKtarmPlanTROumjKQa6Y/R9ePHvdkH3IMHTKI9LRlLF0y3dPtxkoeROd9E052hLdYYOcofZaQkMDgVwbStVtvzmzanl69etCkSeO4yrQylvw8PzPXjZ3NF73/fdiy5W9O5ONLH+PjTo+zefoSzr7/SgDKHlOR8wfewpQ+L/LRxX/lf3f8J+L8UO++O4bLu97g6TZjKS8a75vCaIS3WOBFRZkoIkNFZJWITBWRCiJyu4gsFJFlIjJORCpCbktxsIjME5Hvc1qN4nhVRNaKyP+A48MFisjFIrJERFaIyDsiUs5dvklE/uUuXyAijdzlNd39WOjezneXP+U+f5a7P3/24PU4TKuWzdmwYRMbN24mIyODMWMmcEW3Tl7HRDXTyljy8/zM3D5/LQf27DtsWca+Q3OmlqlQDlXnI/GkHuex6YuF7E//CYA/fvo14vxQc+bO5+fdezzdZizlReN9Uxp4UVE2Bl5T1dOBPcDVwMeq2lJVmwKrgdtC1q8DtAW6As+5y64ETgFOA24CzisoTETKA8OBXqp6Js551rtCVvnFXf4q8LK77BXgJVVt6e7fWyHrnwp0AloBT4pIUjHKXqjklNpsSUvPvZ+2dRvJybW9jIh6ppWx5OdFI7PFI9dy/YJXaHTleSx+YRwAxzasTdljK3H52MfpMekfNL66rW/58Sga75vC2KFXx0ZVXer+vBioD5whInNEZAVwA3B6yPqfqGq2qqYCtdxlFwIfqGqWqqYDM8LkneJmrnPvj3Cfn+ODkP/buD9fArwqIkuBT4FjRKSy+9hEVT2gqruAHSH7ZIzx0aJ/jeWDVvexfvw8TutzKQAJZRKocVYDptz0Al/c8DzN7+/BsQ2i+0FvIpMtkd1igRcV5YGQn7NwWnjDgXvclt3TQPkC1vfjZdB8fk4AzlXVZu4tRVVzjgXlt/9HEJG+IrJIRBZlZ+8v8s6kb91OvbrJuffrptQhPX17kZ9/NILOtDKW/LxoZQKsHz+PBp1bArB/227SvlxO5u8HOLB7H9vnr6H6aSf4vg/xIlq/w3Cy0YhuscCvzjxVgG3uYcyinMmeDfQSkUQRqQOEu7DdWqB+zvlH4Ebgy5DHe4X8/7X781Tg3pwVRKRZEfbpMKo6RFVbqGqLhIRKRX7ewkVLadSoAfXr1yMpKYmePbvz2edTixtfLEFnWhlLfl7Qmcc0OHTgpn6ns9mzYRsAP0xZTO2WpyCJCSSWL0vNZiexZ316QZsxeUTjfVOYeOjM49c4yieA+cBO9/8qhaw/HugApAKbOVTBHUFV/xCRPsBYESkDLATeDFmlmogsx2kpXu8u+zPwmru8DE7FfGdxC3U0srKyuO/+AUyaOIrEhASGjxhNauq6wp9YgjKtjCU/z8/M9q/2I7lNE8pXr8z1Cwfz7aBx1OvQlGMb1kFV2Ze2i7mPDgNgz/p00mYt5+ppz6LZ2az9YBa716ZFvA853hv5Ghdd2IYaNaqz6ftFPP33Fxg2/EPPth/tvGi8b0oDyeltFg9EZBPQwj3f6JsyZVPi50UzxiNvHB/uQJA/7toxM/DMeJd5cKunp8Qerf+niD4vn900KupnKm1mHmOMMb6JlfOMkYjpilJExgMN8iz+i6pOyW99Va3v+04ZY4wpspJfTcZ4RamqV0Z7H4wxxpRuMV1RGmOMKdliZdKASNhcr8YYY3zj9zhKdxrSHaEX6BCR6iIyTUS+c/+v5i4XdxrV9SKyXETOLkoZrKI0xhjjmwDGUQ4HLsuz7K/AdFVtDEx37wN0xpl2tTHQF3ijKAFWURpjjPGN33O9qups4Oc8i7vjTG+K+3+PkOXvquMboKo7yU1YVlEaY4yJWaHTh7q3vkV4Wi1V3eb+vJ1Dc3inAFtC1ktzl4VlnXmMMcb4RiMcIKKqQ4AhETxfRSSinbCK0hhjjG+i1Ov1RxGpo6rb3EOrO9zlW4F6IevVdZeFZRWlMcYT0ZhO7u7kYK9X+ea2rwLNA8gu4dOMRmlmnk+Bm3GueXwzMCFk+T0i8iHQGuf6xdvy38QhVlEaY4wpsUTkA6AdUENE0oAncSrIMSJyG/AD0NNdfRLQBVgP/Ab0KUqGVZTGGGN843d7UlWvL+Chi/NZV4F+xc2witIYY4xvbFJ0Y4wxJox4mMLOKkpjjDG+iXR4SCywCQeMMcaYMKxFaYwxxjd26NUYY4wJIx4OvVpFaYwxxjfWojTGGGPCKOkzC0EJ6MwjIptEpEYEz58lIi3cnyeJSNUw694pIjcdbVZBOnVsx6qVs1mTOpdHHi72WNcSkWllLPl50cgMIu+iPp3565QXeHTqC7S7tQsAyU1O5IGP/8FfJ/+bvm89QvnKFXzJLleuHF/N/ZxFC6eydMl0/vbEQ77khIrG+ybexXxF6SVV7aKqe8I8/qaqvutlZkJCAoNfGUjXbr05s2l7evXqQZMmjb2MiHqmlbHk50UjM4i8OifXo811FzOo+2M83/kRTu9wNjVOrMX1z93BZ8+P4rnLHmb5lAV06NvN09wcBw4coGOnnrRo2ZEWLTvRsWM7WrU625csiM77pjABXLjZd55WlCJSX0TWiMhwEVknIu+LyCUi8pWIfCcirdzb1yKyRETmicgp7nMTReQFEVkpIstF5N6QTd8rIt+KyAoROdVdv5KIvCMiC9xtdXeXVxCRD0VktYiMByqE7F9u61REbnJzlonISHfZUyLS38vXpFXL5mzYsImNGzeTkZHBmDETuKJbJy8jop5pZSz5edHIDCKvVqMUflj6HRl/HCQ7K5v181Npellrjm9Qh/XzVwOwZu4KmnVu7WluqP37fwMgKakMSUllUB8PRUbjfVOYbDSiWyzwo0XZCBgEnOre/gS0BfoDjwFrgAtUtTnwN+AZ93l9gfpAM1U9C3g/ZJu7VPVs4A13OwCPAzNUtRXQHvi3iFQC7gJ+U9UmOJPjnpN3B0XkdGAA0EFVmwL3eVP0IyWn1GZLWnru/bSt20hOru1XXFQyrYwlPy8amUHkbVu7hZNankrFqpVJKl+W09o3p2qd49j+3RbO7NgCgOZdzqVqneM8zQ2VkJDAwgVT2Jq2jOnT57Bw4RLfsqLxvimMRvgvFvjRmWejqq4AEJFVwHT3wpkrcCrCY4ERItIYp2Wd5D7vEuBNVc0EUNWfQ7b5sfv/YuAq9+eOwBUhLcDywAnAhcBgdxvLRWR5PvvYARirqrvyycqXe1XtvgCSeCwJCZUKe4oxJsp+3LCV/735Kf1GPs6B3w6wNXUTmp3N+4+8yTVP3sJl917Niv8tJisj07d9yM7OpmWrThx77DGMHfMWp592CqtS1/qWZ7znR0V5IOTn7JD72W7eP4CZqnqliNQHZhVjm1kc2mcBrlbVw95xInJ0e12I0KtslymbUuSvOelbt1OvbnLu/bopdUhP3+79DkYx08pY8vOikRlU3jdjZvLNGOdamV0fvo49235mx4Z0Xr/JOZhVs0EdTm/f3PPcvH755Ve+/HIeHTu1862ijMb7pjDxMDwkGp15juXQFaVvCVk+DbhDRMoAiEj1QrYzBefcpbjr57zTZ+Mc7kVEzgDOyue5M4BrReS4ImYdtYWLltKoUQPq169HUlISPXt257PPp/oVF5VMK2PJz4tGZlB5lY87BoBqycfR9LJWLP50bu4yEaHTPVfx1fvTPM8FqFGjOsce62SVL1+eiy++gLVr1/uSBdF53xQmHs5RRmMc5b9wDr0OACaGLH8LOBlYLiIZwFDg1TDb+Qfwsrt+ArAR6IpzHnOYiKwGVuMcrj2Mqq4SkYHAlyKSBSzh8ErbM1lZWdx3/wAmTRxFYkICw0eMJjV1nR9RUcu0Mpb8vGhkBpV32xsPUqlaFbIysxj7xDv8/utvXNSnMxfc2BGAZVMW8M3YWZ7nAtSpXYu3336JxMREEhKEjz76nEmTpvuSBdF53xQmVs4zRkL87IEVr4pz6NUY45+7k9sGmvfmtq8CzYPgB+xnHtzq6fmrq068IqICfPzDp/6cTyuGUjWO0hhjjCkum8LOGGOMb+LhqKVVlMYYY3wTKx1yImEVpTHGGN/Ew/AQqyiNMcb4Jh56vVpnHmOMMSYMa1EaY4zxjZ2jNMYYY8KwXq/GGGNMGPHQmcfOURpjjDFhWIvyKCT4dIWSgiQlBvtrOpCZEWheaVEmITGu85Ir+XdNx4JM/31ToHn7t84ONA+g6gkdAs/0Ujz0erWK0hhjjG+sM48xxhgThnXmMcYYY8KIhxaldeYxxhhjwrAWpTHGGN9YZx5jjDEmjKAvPO0HqyiNMcb4puRXk1ZRGmOM8ZF15jHGGGPiXNxVlCKySURqRHs/8kpISGDB/MmMHz88kLy77+7DwoVTWLhoKv363ep73tAhg0hPW8bSJdN9z8rRqWM7Vq2czZrUuTzycL+4yzv22GMYNepNli2bwdKl02nd+mzPM15/83k2blrIgoWTc5dVq3Ysn342kqXLZ/DpZyOpWvUYz/LKlivLR1NG8OnMUUycM5o/P9IXgOf+8yTTF01gwsz3mTDzfZqccbJneR9OfoePZ7zHhC8/oN/DtwPQuu05jJ02gk++HMUzg/9GYmJksxgdOHCQ6/7vPq66+W6633AHr741EoDH/zmITtfcwtU39+Pqm/uxZt2Gw563YvVaml54OVNnzokoP1TQf/uFyUYjuhWF+7m/QkSWisgid1l1EZkmIt+5/1c72jLEVEUpjpjaJ6/ce+9trFmzPpCs0047mT59ruPCC7tzbuvOdO7cgYYNT/Q18913x3B51xt8zQiVkJDA4FcG0rVbb85s2p5evXrQpEnjuMkDGDToKaZNm0XTph1o2fIyX94/748cR48etxy27MGH7mLWrK9odlYHZs36igcfusuzvIMHDnLTVXdyRfs/0b39n7igw3k0PecMAP719GC6t7+B7u1vYPXKdZ7l3XpVP67q0JurL+5N2w7n0qzFmQwc/CT97xhAj4v+RHradrr36hJRTtmySbwz+Dk+HvE6H414ja/mL2bZytUAPNTvNsaNeI1xI17j1JNPyn1OVlYWL70+jPNaevcFKBp/+4VR1YhuxdBeVZupagv3/l+B6araGJju3j8qUa+URKS+iKwVkXeBlcDbIrJIRFaJyNMh620SkadF5Fv3m8Op7vLjRGSqu/5bgIQ850ERWene7g/JWyMiw0VknYi8LyKXiMhX7jePVl6XMSWlDp07X8w7w0Z5vel8nXJKIxYuWsrvv/9BVlYWc+bOp3v3y3zNnDN3Pj/v3uNrRqhWLZuzYcMmNm7cTEZGBmPGTOCKbp3iJu+YY6rQtm0rhg37EICMjAx++eVXz3O++moBu3/ec9iyy7teyvvvjwPg/ffH0bVbR08zf9v/OwBlkspQJqmM7zO3/PZbSF6ZMmRlZ5ORkcEP328BYN6XC7j08sjmUxURKlasAEBmZiaZmZlIIXNCj/roUy5tdz7Vq1WNKDtUNP72CxNEi7IA3YER7s8jgB5Hu6GoV5SuxsDrqno68JD7jeAs4CIROStkvV2qejbwBtDfXfYkMNd97njgBAAROQfoA7QGzgVuF5Hm7nMaAYOAU93bn4C27jYf87pwg154ikcfHUh2djAntVNT13LeeS2pXr0qFSqUp1On9qTUrRNIdlCSU2qzJS09937a1m0kJ9eOm7z69euxc+fPDB06iG++mcQbbzyf+0Hst+OPr8GP23cC8OP2nRx/vLdnMhISEpgw832+Xj2Nr2bNZ/m3qwB44LG7+XTWBzz6jwdJKpvkad646SOZs2oyX3+5gBXfrqJMYiKnNz0VgI7dOlA75fiIc7Kysrj65n5c2PV62rRszlmnO9sf/N8RXHnTXTz/yn85ePAgAD/u3MX02fPodeXlEeeGise/fRHp6zaecm5981lNgakisjjk8Vqqus39eTtQ62j3IVYqyh9U9Rv3554i8i2wBDgdOC1kvY/d/xcD9d2fLwTeA1DVicBud3lbYLyq7lfVfe5zL3Af26iqK1Q1G1iF0zxXYEXIdj3RpcvF7Ni5iyVLVni52bDWrt3Aiy++yaefjeSTCSNYvjyV7Kx4uCpc6VGmTBmaNz+DIUNGcu65Xdi//3cefvjuqOyL1y2+7Oxsure/gQvP6sJZZ59O41NPYtA/X+WyNldzdcebqFrtGPree7OneVdffCMdmnXjzLNPp9GpDel/5wD+8vcH+HDyO/y2b78nfx+JiYmMG/Ea08ePZEXqOr77fhP339mHzz4Yyui3XuGXX/fy9ntjAXj+lf/ywF23kpDg7UdwLP7ta6T/VIeoaouQ25B8Ytq6jajOQD8RufCwfXDexEf9Ro6VinI/gIg0wGnVXayqZwETgfIh6x1w/88isqEtB0J+zg65n13QdkO/1WRn7S9y0HltWtL18o6sW/s17418jfbtzmf4sMFHveNF9e6IMbQ9vxudOvZiz55f+G79975nBil963bq1U3OvV83pQ7p6dvjJm/r1m1s3bqNhQuXAjB+/CSaNTvDt7xQO3bsolbtmgDUql2TnTt/8iVn76/7mD93ERd0aMPOH52MjIMZjBv1GWedfboveQvmLqZt+zYsW7SSm7rfwXWX3cqir5eyacNmz3KOqVKZVmefxdxvFlGzRnVEhLJly9Lj8o6sWO2ce1215jsefvI5Ol59M1NnzeWfL7zG9NnzPMmPtb/9IM5RqupW9/8dOEcWWwE/ikgdAPf/HUdbhlipKHMcg1Np/iIitXC+HRRmNs6hU0SkM5DTs2kO0ENEKopIJeBKd9lRCf1Wk5BYqcjPG/DEczQ8qSUnn9KG3jf2Y+asr7ilz5+PdjeKrGZN59qAdesmc8UVlzFm9Ke+ZwZp4aKlNGrUgPr165GUlETPnt357POpcZP34487SUvbRuPGDQFo3/58Vq/+zre8UJMm/o8bbrgagBtuuJqJn0/zbNvVjqtKlWMqA1CufDnOb9ea77/bRM1ah65leUmXi/huzYaCNhFRXpuLWrFx/Saq13A+JpLKJnHbvTcy5t2Pw22mUD/v3sOve/cB8MeBA3y9cAkNTqzHzl0/A05lMWP2PBq7HWumfDScqeNGMHXcCDq2a8uA/v24+MLzItqHHLH2t+/3OUoRqSQiVXJ+Bjri9Hf5FMg5NHEzMOFoyxBTEw6o6jIRWQKsAbYAXxXhaU8DH4jIKmAesNnd1rciMhxY4K73lqouEZH6nu94DHp/1BtUr16NzIxMHnzgCV86goR6b+RrXHRhG2rUqM6m7xfx9N9fYNjwD33Ly8rK4r77BzBp4igSExIYPmI0qane9JSMhTyABx74G8OHD6Zs2SQ2btxM3779C39SMQ0b/goXXHguxx1XjbXfzWPgP1/mxUFv8O7IV7np5p5s2byVm268x7O842vV4PlXnyYhIYGEhAS+mDCNWdPmMuLjN6h+XDVEhNUr1/Lkw896klezVg2eGfw3EhKdvCkTpvPltK946G/3ctGl55OQkMDoER8zf+7iiHJ2/rSbx//5AlnZ2Wi20qnDBbQ7vzW33vtXdu/5BVXllMYNefLhez0pVzhB/+0XJoDLbNUCxrudp8oAo1R1sogsBMaIyG3AD0DPow2QeLhWWNDKlqsb6IuWlBjs95kDmRmB5pUWZRIiG6sX63nJlY4rfCWPJQVcxqWrPgg0D6DqCZH1yC2u/b9tCt9dt5ia1z4/os/LJdu/8nR/jkZMtSiNMcbEl3iYws4qSmOMMb6xy2wZY4wxYdhltowxxpgw4qFFGWvDQ4wxxpiYYi1KY4wxvrFDr8YYY0wY8XDo1SpKY4wxvomHFqWdozTGGGPCsBalMcYY39ihV2OMMSaMeDj0ahXlUQj6F29zr8aHoL9Z/5F5MNC873/ZVvhKJVyF5AsKX8ljI2q0DzzTS9aiNMYYY8JQLfkXjbfOPMYYY0wY1qI0xhjjG7t6iDHGGBNGPFzz2CpKY4wxvrEWpTHGGBNGPLQorTOPMcYYE4a1KI0xxvjGJhwwxhhjwoiHCQcCP/QqIn8WkdUi8v5RPHe4iFzjx375qVPHdqxaOZs1qXN55OF+cZlpZfTe2rXzWLxoGgvmT2beVxN9z4P4f02HDhlEetoyli6Z7nuW33nnvng7Vy9/jctnPHvEY6fe0Zkb0t+jXPXKhy2v3rQh128eQb3LW3q+PwVR1YhusSAa5yjvBi5V1RuikB24hIQEBr8ykK7denNm0/b06tWDJk0ax1WmldE/HTv1pFXryzjv/Mt9zyoNr+m7747h8q7BffT4mff96NnMuOHfRyyvmFydOhedyf60XYctlwSh+eO92PblCl/2J54FWlGKyJtAQ+ALEXlARJ4Skf4hj68UkfruzzeJyHIRWSYiI/PZ1j/cFmZiyLKTROTbkPuNc+6LyMUiskREVojIOyJSzl2+SURquD+3EJFZXpa5VcvmbNiwiY0bN5ORkcGYMRO4olsnLyOinmlljA+l4TWdM3c+P+/e42tGUHk75q/l4O59Ryw/56neLPnnh0e0xk6+tSNbJi3kj12/+rI/BclGI7rFgkArSlW9E0gH2qvqSwWtJyKnAwOADqraFLgvz+P/BmoCfVQ1K2T7G4BfRKSZu6gPMExEygPDgV6qeibOudm7vCpXOMkptdmSlp57P23rNpKTa8dVppXRJ6pM/Px9vp43kdtu+5O/WZSS1zTO1e10Nr9t382e1M2HLa9Quxr1Ordg3YhgDjmHiodDr7HamacDMFZVdwGo6s8hjz0BzFfVvgU89y2gj4g8CPQCWgGnABtVdZ27zgigH/CyD/tujCfad7ia9PTt1Kx5HJMmjmLt2g3MnTs/2rtlYlRihbKcfu8VzLj++SMeO+fp3iwZ+CFEoeKxXq+Ry+TwVm35IjxnIXCOiFTPU4HmGAc8CcwAFqvqTyJSt4j7UGC+iPQF+gJI4rEkJFQqwq5C+tbt1KubnHu/bkod0tO3F+m5RyvoTCujT5nu9nfu/IkJn06mZYtmvlaUpeE1jWdVTjyeyifUpMv/ngGgYp3qdJ7yTyZ3eZLjmjag7Rv3AFCuehVSLm6KZmWTNnmx7/sVK63CSER7woFNwNkAInI20MBdPgO4VkSOcx+rHvKcycBzwEQRqZJ3g6r6BzAFeAMY5i5eC9QXkUbu/RuBL0P24Rz356sL2lFVHaKqLVS1RVErSYCFi5bSqFED6tevR1JSEj17duezz6cW+flHI+hMK6P3KlasQOXKlXJ/vuTiC1m1aq1veRD/r2m827MmjXFn9WNC6weY0PoBftv2M190GsAfO39hwrkP5i7f/PkCFjw6PJBKMl5Eu0U5DrhJRFYB84F1AKq6SkQGAl+KSBawBLgl50mqOtatJD8VkS6q+nue7b4PXAlMddf/Q0T6AGNFpAxOq/RNd92ngbdF5B/ALK8LmJWVxX33D2DSxFEkJiQwfMRoUlPXFf7EEpRpZfRerVo1GTN6KABlyiTy4egJTJ02y7c8iP/XFOC9ka9x0YVtqFGjOpu+X8TTf3+BYcM/LJF557/ej1ptmlCuemWuXDSY5YPGseGDLwt/YsBipUNOJCQemsV5uT1pj1XVJ/zYfpmyKfH3ohnfJSYEewAnK7vkXzDXwIga7QPNuyH9PfFye8dUahjR5+Wv+7/3dH+ORrRblJ4TkfHASTgdgowxxkSRdeaJQap6ZbT3wRhjTPyIu4rSGGNM7IiHuV6tojTGGOMbO/RqjDHGhBEPHUatojTGGOObeDj0Gu0JB4wxxpiYZhWlMcYY3wQxKbqIXCYia0VkvYj81esy2KFXY4wxvvH7HKV7qcXXgEuBNGChiHyqqqleZViL0hhjjG80wlsRtALWq+r3qnoQ+BDo7mUZrEV5FDIPbj2qKZVEpK+qDvF6f2IlLxqZ8Z4XjUwrY8nPi1Zmfo728zJH6JWbXEPylCsF2BJyPw1oHUlmXtaiDFZB19CMl7xoZMZ7XjQyrYwlPy9amZ4LvXKTewu88reK0hhjTEm2FagXcr+uu8wzVlEaY4wpyRYCjUWkgYiUBa4DPvUywM5RBivoQwbROD8R72W01zQ+MuM9L1qZgVPVTBG5B5gCJALvqOoqLzPi8nqUxhhjjFfs0KsxxhgThlWUxhhjTBhWURpjjDFhWGceH4nIOaq6OM+yrqr6ebT2yZQMInIMoKq6N9r7YkoGEZlJPpPZqGqHKOxOXLHOPD4SkW+Bm1R1pXv/euB+VfV01oiQvI+Bt4EvVDXbj4yQrAfDPa6qL/qYXRO4HahPyJc9Vb3Vx8yrgLY4H0RzVXW8TzktgXeAKoAAe4Bb837h8jgzmr/LRKAWh/8eN3uc0VtV3yuonH6VT0RaAI8DJ+KUT5w4PcunvHNC7pYHrgYyVfURP/JKE2tR+usa4CMR+RNwAXAT0NHHvNeBPsBgERkLDFPVtT5lVfFpu0UxAZgD/A/I8jtMRF4HGgEfuIvuEJFLVLWfD3FvA3er6hw3uy0wDPDlw9WV87s8BWjJoTFo3YAFfoWKyL3Ak8CPQM4XO8X7slZy/w/6Pfs+8DCwgkPl800+X6a+EhHffn+libUofSYiJwOfAJuBK1X19wAyjwWux/k2uwUYCrynqhl+ZwdBRJaqarMA89YATdT9YxGRBGCVqjbxIWuJqjbPs+xbVT3b66x8smcDl+cc7hWRKsBEVb3Qp7z1QGtV/cmP7UebiMxV1bYB5lUPuZsAnAMMVtVTgtqHeGUtSh+IyAoOP1dQHWcg7HwRwa9DL272cUBv4EZgCc632rbAzUA7D3MGh3tcVf/sVVY+PheRLqo6yceMUOuBE4Af3Pv13GV++FJE/ovTelWgFzBLRM4GUNVvfcoF5xDowZD7B91lftkC/OLj9g8jIiOA+1R1j3u/GjDIx0P2T4rIW8B04EDOQlX92Ke8xTjvGQEygY3AbT5llSpWUfqja8jP1XAOuyrO4cI9foWKyHicw2cjgW6qus19aLSILPI4zrdzZkVwH/CYiBwAMjh07ucYn/KqAKvdw1iKc1mfRSLyKU7wFR5mNXX/fzLP8uZutp8dM94FFrjvI4AewAgf877H+RIwkcMrEr/OiZ6VU0m6ObtFpHmY9SPVBzgVSOLwQ8u+VJSq2sCP7RqrKH2hqj8AiMifcTqdfIzzYT4S5zDof7zOdA8HLlbVKwvYpxZe5qmqnx+ghWUHfa7pb0EFqWr7oLLyyR4oIl/gfLED6KOqS3yM3Ozeyro3vyWISDVV3Q25hyr9/AxsGeRhTxFJAu4Ccg6VzwL+Gy+nXKLJzlH6SESWA21Udb97vxLwtY+93o44v+U3twfqX4DTcHraAf50SReRU1V1Tc5hyLx8PiwZCBHJt1JW1b8HlN8WaKyqw9zfbWVV3ehzZkVV/c3PDDfnJuAxYCzOF9drgIGqOtKnvGHAv1U11Y/t55P3Fk7rNedL7I1Alqr+XxD58cxalP4SDu+VmeUu88t0Ebka+FiD+wb0PjAauBy4E+dc6E6fsh7EucbeoHwe8+2wpIjs5dA557I4H0b7fTrUuz/k5/I4h/FX+5BzBBF5EmiBc/h+GE453wPO9ymvDU4v38rACSLSFLhDVe/2I09V33VPQeS8T67yuRI7F1gqIhtxDi37OjwEpwXbNOT+DBFZ5lNWqWItSh+547ZuBkLP+QxX1Zd9ytuL0xU+C/gd/8/dISKLVfUcEVme8wEgIgtVtaVfmUXYp0tVdZpP2xagO3Cuqv7Vj4w8eeWAKaraLoCspTjnQr/NOTIR+nv1IW8+Tqvu05C8lap6hsc5x6jqr3l6heZS1Z+9zAvJPbGAvB/yW+5B3rfAtaq6wb3fEPgoiB7T8c5alD5S1RdFZBZOr1Pw+ZxPFM7dgdOZBmCbiFwOpOP08o2m5wFfKkq3pf6J2/ryvaIEKuJciDYIB1VVRSRnGEylwp4QKVXd4nz3yOXHuNhROC3znF6hOcS939CHTFT1B7eVnHPOd46q+tnCexiYKSLf45TtRJwORSZCVlH6zD1vFti5MxG5gpCT+QFMl/dPd9zmQzidlI4B7vc5szCeHt52Z+XJkYBzePIPLzNCskKHFiUCNYFAzk8CY9yhKVVF5HbgVpzOZ37ZIiLnAep2RLkPHw4zq2pX9/9Ae4WKyH0c6swH8J6IDFFVzzvzAajqdBFpjHPoHGCtqh4I9xxTNHboNY6IyHM4M6u87y66Hlikqo/6mJl3bFp14AU/p5Mrwj55OkDf7ZSRIxPYBAxV1R1eZYRkhR6uywR+VNXMkMdze236QUQuxZk9SnAO+frSMnezagCvAJfgfAGZgvNe8nQCgoI6f+XwqxNYFDrzLcY55/uBn++R0sgqyjji/mE2U3eeV3cezSU+T3CQ30wygfe+zZMfyEw2IXmPquqzAWX5Vjb3g/wPVc0SkVNwWiZflPThBeJMFg5O56gWwDKcLwJn4XyRbONT7gqcDjZ/uPfLAwtV9Uyf8hrhHGrtBSzC6ZA1NcCOfXHLLrMVf6qG/HxsAHkJ7gwnQCBj04piU8B51waY5Wev6dlAORFJASbjDC8Y7leYiDQUkc9EZKeI7BCRCW4HFE+pant3fOo24GxVbaGq5+B0XNrqdV6IYTizcT0lIk8B3+C0+HyhqutV9XHgZJzzsu8AP4jI0wV1ZDJFE+0PNOOtZ4El7jdowTlX6dthV9cg4GtxJmEHp9IY6GdgnnOGOX4BVqjqDlXN73FfdynALD9bB6Kqv4nIbcAbqvovtyesX0YBrwE5k2RchzN1ny9X1wFOUdUVOXdUdaWIeD5fb8j2A+3MByAiZ+G0KrsA4zg0heUMoJmf2fHMDr3GGRGpg3OeEmCBqm4PIPM0Do1Nm+H3AGt3yrM2QM4htXY4PRobAH/3awB5mP0J7FCvz4delwB3Ay8Bt6nqKhFZ4eOhwiOGnojIsjxjAb3M+wBnnOp77qIbcCZUuN7jnLCtNx+HoyzGmSLzbWBcaEceEfk4Cl8g44a1KOOIiExX1Ys5dJmk0GW+cSvGQGYfcZXBuZrHjwAiUgtnntLWOIcPA60oCbZF6WfW/ThHIMa7lWRDDn0Z8cMXIvJX4EMOTQA/Kaei8aFC6YMzxdt97v3ZwBseZ8Dhk5OfAOx2f66KM2WfX71vr1XV7/N7wCrJyFiLMg64nQQq4nyotePQh+kxwGRVPTVKu+YLEUlV1dNC7gvOZa9Oi0ZHIhF5TFWf8XB7BY69E5HqfrVIQjIqA6jqPp9zwk2Np6rq+flKEakAnKD+Xac1NGsozpeOSe79zkAPVb3D72zjLevMEx/uwPkWe6r7f85tAvBqFPfLL7NE5HMRuVlEbsYp5yy31+Yer8NE5GQRmS4iK937Z4nIgJzHPa4k78M5r3S8e3tPnAsc52T5VkmKyJnu4ddVQKqILBaR0/3KU9UGYW5+VJJXAEtxOiohIs3EvQKMT87VkEvBqeoXwHk+5hmfWIsyjojIvX4NZo4lbgvyag7NQfoVzjkZX97MIvIlzqwn//VzqjV3u4GOvcuTPQ94XFVnuvfbAc+oqi8f7hLw1S7cc3gdcCbiyPk9+nkOdgrOpfVCz4leqKqdfMqTvH8DIlLOJh2InLUo40u2iFTNuSMi1UTElwmmo0kdH6nqA+7tI5/HilVU1QV5lmXmu2bkgp5IP1SlnEoSQFVn4cwd7Jc3gHOA193bOfhzzjBHhqrmvVC0n++b63FmVhrv3o53l/nlsKEn7iH0oC5uHtesM098uV1VX8u5o86FaW/H+RCKG+7wkOdxPngE/yd/3yUiJ+F+qIrINThj8vyQM/YudCJ938be5fG9iDzBoc5QvXEuruyXoK92sUpE/gQkijPV25+BeX6FuYfJ7yt0Re+kicjrqnq3O7Z5Iv5OQVhq2KHXOOLOBHJWTuvKnZlnuar6dp4pGkRkPdBNVYO6/FRDYAjO+aXdwEagt6pu8invbA6NvZvj99i7kNxqwNOh2cBT6tN0aBLw1S5EpCLwOCFT9AH/yJk5x4e8k4H+QH1CGiXqw7VaQzL/hdOJ7xzgOVUd51dWaWIVZRwRkX/jXDHgv+6iO4AtqvpQ9PbKeyLylar6co3EQnIrAQmqutfHjH/gDFuYl3OeMl6JyMU4LejDrnYRevi3JHNbx2/idKzLPZyuqos9zgkd+iHAE8AC3E5Lqvpxfs8zRWcVZRwRkQScyjFn3OQ04C1V9ePSRVEjIq8AtYFPcC6IC/j3gSDONSGv5siWgedX9RCRPjhDQ9oAe3FadbNVdYLXWSGZnxHmXJ2qXuFjdjkKuNqFeHxd0aBbeOJeq9WPbefJGRbmYdUoXqAgXlhFaUqcAj4YfPtAEJHJOFPk5W0ZDPIjz82sDfTE+WCvpj5ea1RELgr3uKp+6Vd2OF7PQhRUCy8k7ylgB05HntAvdL6OgzXes4oyDojIGFXtKYdfyzBXEEML4plfQ0EKyHoLOA34Eac1ORf4VkMuteVjdiXgdz386jPlVPU3v7ML2B9PJ48IqoUXkpffhAq+TKTg5uW95F01YJC1KCNnvV7jQ07PuuE4VyhIi96u+M+dieg24HScSycB4OMHwjwROVNDJtT20XE4F2zeA/wM7AqiknRNx7k2ZM6MPBWAqURvkLwn3+JD5l79zB0uFUgLTwO+UDROR749Ifm7RSRql7uLJ1ZRxgFVzRmqUBmnd+bPwGhgrLrzocaZkcAaoBPwd5yB3H72gG0L9BGR73E+YHOGo3jeUlfVKwHEuapFJ2CmiCSqal2vs/JRPnTaOlXd5/YULelC514FZ/KIHAr40sIDEJEzcI4QhH6he9enuAQJubC3xMYl7+KCvYhxRFWfBp4W51I7vYAvRSRNVS+J8q55rZGqXisi3VV1hIiMwjlM6ZfOQDUOzb86Gx+mygMQka5uzoU4k2jPwN+yhdovImer6rfuvpwD/B5Qdn42ebGRKLTsABCRJ3HmXj4NZ+B/Z5xD6X5VlKGXvBPgGny+5F1pYRVlfNoBbAd+whmUH29ypjjb435j346/5ewB/B/wMc4H0Eicgdx+TBd4Fc74vldUNR1ARJ73ISc/9wNjRSQdp5y1cb5weUryv55orpzey+rxFS9EpB/wfp5zeNerql8TclwDNAWWqGofca5y814hzzlqqvquiCzi0CXvrlKfL3lXWlhnnjjinn/piTNt1lhgTDz+oYjI/+FclPZMnPOylYEnVPW/4Z4XQV5g86/m19NT8rluo1/c+VdDh2tkhDzmyXCNkF7Lx+Oc/5zh3m+PM360a6QZBeQuVdVmeZZ52mEoz7YXqmpLd47Z9jjDfVarT1fzEZET8luuqpv9yCtNrEUZX+oB96vq0mjviM9Gcmhc4wh3WS0f83yff1VE7sK5aHJDt2LOUQVn0vdAuBXjygIefh5nbG6kGX0ARGQqcFrOOXZxLjo+PNLth5EYOnG426u3rB9BIiLAcnfu5aE450n3AV/7keeayKEOUBVwrnu5FqfTm4mAVZRxRFUfjfY+BGQCh8Y1BnFlhCDmXx0FfAE8C/w1ZPneGBp35/Xk7PVCOqKBMyQm31aRRyYDo0UkdOaqyX4EqaqKSCv3MO+b7ljcY1R1eSFPjSTzsKuguFMhxt1FEaLBDr2aEifIcY0hmVGZfzWW+DABwKtAY+ADd1EvYL2q3lvwsyLKC3TmKndc46uqutCP7RdxH3y7jFhpYhWlKXFEZAjwn4DGNRqX1xWlu82rCOlNrKrjw61fkojIGqAR8AOwHx+HFbl5D4bcTQDOBo5Tn65/WZpYRWlKjJCZh8rgtER8H9doDhGRj73uiRok99Jaz3LkuEa/Zso5Mb/lqvqDT3lPhtzNxBleM059ujpKaWLnKE1J4ktvyNIuisM1gr6u6DDgSeAlnF6offDx4vV+VYh5ichIVb0R2KOqrwSRWdpYi9KYUi6KwzWCvq7oYlU9J/S8XdDzv/pBRFJxph78AmeCg8M6XcVQZ7ASy1qUxpRyURyu8WNQlaTrgNuh5zsRuQfYijMGt6R7E2ee3oY4PcFDK0pfp+grLaxFaYwBQERWq2qTkPsJwKrQZR7nBX1d0ZY4cwJXBf4BHAP8S1Xn+5EXNBF5Q1XvivZ+xCOrKI0xQFSGawR9XdEWwOPAiUBSSJ51AjNhWUVpjMkV58M11uJcOWQFkJ2zPKhON6bksorSGBMVQV9XVETmqmrbwtc05nC+dY02xpQsInKViHwnIr+IyK8isldEfvUxciTOOcpOwJdAXZyJw/3ypIi8JSLXu2W9qrChMcaAtSiNMa4oDNdYoqrNc66O4l65ZI6qnutT3nvAqcAqDh169e2cqIkfNjzEGJMj6OEaQV9XtKWqnlL4asYczipKY0yORSIymoCGawBD3IsnDwA+xb2uqE9ZAPNE5LR4vEar8ZcdejXGAFEZrlGOQ9cVDR2u8Xef8lYDJwEbsTmCTTFYi9IYAxyaoSdAQV9X9LIAMkwcshalMQaIynCNwK8raszRsOEhxpgcQQ/XmCcidlFhE/OsRWmMAYIbrmHXFTUljZ2jNMbkCGq4hl1X1JQoVlEaY3IEMlzD5lY1JY0dejXGAMEP1zCmpLAWpTEmR9DDNYwpEaxFaYwBbLiGMQWx4SHGmBw2XMOYfFiL0phSzoZrGBOeVZTGlHIicmK4x62XqintrKI0xhhjwrBzlMYYY0wYVlEaY4wxYVhFaYwxxoRhFaUxxhgThlWUxhhjTBj/D8t9eTbITDWZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 504x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94       115\n",
      "         1.0       0.75      0.98      0.85       132\n",
      "         2.0       0.99      0.89      0.94       132\n",
      "         3.0       1.00      1.00      1.00       173\n",
      "         4.0       0.96      0.63      0.76       106\n",
      "         5.0       0.83      0.85      0.84       157\n",
      "         6.0       0.91      0.91      0.91       159\n",
      "         7.0       0.78      0.99      0.87       138\n",
      "         8.0       0.69      0.97      0.81       102\n",
      "         9.0       0.92      0.76      0.83       467\n",
      "        10.0       0.94      0.95      0.95       151\n",
      "\n",
      "    accuracy                           0.88      1832\n",
      "   macro avg       0.88      0.90      0.88      1832\n",
      "weighted avg       0.89      0.88      0.88      1832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, dict_keys=None, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    if dict_keys:\n",
    "        labels = [dict_keys[x] for x in labels]\n",
    "\n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred, {v: k for k, v in classes.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/3w/bsb667zn42gdhbqq3z8ptmgw0000gn/T/tmp0ur8c5m2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 17:09:42.348883: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-07-30 17:09:42.348900: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-07-30 17:09:42.349931: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/3w/bsb667zn42gdhbqq3z8ptmgw0000gn/T/tmp0ur8c5m2\n",
      "2022-07-30 17:09:42.352215: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-07-30 17:09:42.352232: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/3w/bsb667zn42gdhbqq3z8ptmgw0000gn/T/tmp0ur8c5m2\n",
      "2022-07-30 17:09:42.357161: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-07-30 17:09:42.358888: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-07-30 17:09:42.427514: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/3w/bsb667zn42gdhbqq3z8ptmgw0000gn/T/tmp0ur8c5m2\n",
      "2022-07-30 17:09:42.450101: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 100171 microseconds.\n",
      "2022-07-30 17:09:42.502666: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8040"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save as a model dedicated to inference\n",
    "model.save(model_save_path, include_optimizer=False)\n",
    "\n",
    "# Transform model (quantization)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get I / O tensor\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 µs, sys: 48 µs, total: 164 µs\n",
      "Wall time: 149 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Inference implementation\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7063590e-05 1.6513423e-04 1.0740484e-05 1.8660815e-10 2.2817042e-03\n",
      " 7.7829340e-05 6.8759853e-01 3.8016873e-04 1.4390363e-03 3.0795476e-01\n",
      " 7.5038501e-05]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 0,\n",
       " 'victory': 1,\n",
       " 'call': 2,\n",
       " 'hang_in': 3,\n",
       " 'one_up': 4,\n",
       " 'two_up': 5,\n",
       " 'hand_closed': 6,\n",
       " 'hand_open': 7,\n",
       " 'machedici': 8,\n",
       " 'random': 9,\n",
       " 'fuck you': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
