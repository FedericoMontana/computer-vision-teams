{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[92854]: Class CaptureDelegate is implemented in both /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x13d476480) and /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x12dd6c860). One of the two will be used. Which one is undefined.\n",
      "objc[92854]: Class CVWindow is implemented in both /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x13d4764d0) and /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11fc94a68). One of the two will be used. Which one is undefined.\n",
      "objc[92854]: Class CVView is implemented in both /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x13d4764f8) and /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11fc94a90). One of the two will be used. Which one is undefined.\n",
      "objc[92854]: Class CVSlider is implemented in both /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x13d476520) and /opt/homebrew/Caskroom/miniforge/base/envs/env_tf/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11fc94ab8). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "\n",
    "from src.handDetector import HandDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_out = \"out/lala.csv\"\n",
    "\n",
    "def get_init_gestures():\n",
    "    gestures = ['ok', 'victory', 'call', 'hang_in', 'one_up', 'two_up', 'hand_closed', 'hand_open', 'machedici', 'random']\n",
    "    gestures = {ord('a')+x: [gestures[x], 0] for x in range(len(gestures))}\n",
    "    return gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def put_text(img, text):\n",
    "\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  fontScale = 0.5\n",
    "  color = (255, 0, 0)\n",
    "  thickness=1\n",
    "\n",
    "  y0, dy = 15, 15\n",
    "  for i, line in enumerate(text):\n",
    "    y = y0 + i*dy\n",
    "    cv2.putText(img, line, (5, y ), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def warning(img):\n",
    "  img = cv2.circle(img, (int(img.shape[1]/2),int(img.shape[0]/2)), 20, (0, 0, 255), -1)\n",
    "\n",
    "  return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "detector = HandDetector()\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "SHOW_WIDTH = 800\n",
    "NO_KEY = -1\n",
    "ESC_KEY = 27\n",
    "WAIT_KEY = 100\n",
    "\n",
    "gestures = get_init_gestures()\n",
    "results = []\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "  success, img = cap.read()\n",
    "  if not success:\n",
    "    break\n",
    "\n",
    "  k = cv2.waitKey(WAIT_KEY)\n",
    "  if k == ESC_KEY:\n",
    "    break\n",
    "  \n",
    "  img = imutils.resize(img, width=SHOW_WIDTH)\n",
    "  detector.fit(img)\n",
    "\n",
    "  # There is a hand on the screen\n",
    "  if detector.found_hands():\n",
    "\n",
    "    if k != NO_KEY:\n",
    "\n",
    "      lm_norms = detector.get_positions_normalized()\n",
    "\n",
    "      for h_k, lm_norm in enumerate(lm_norms):\n",
    "        # mediapipe tries to guess hands parts if they are not in the \n",
    "        # screen, let's not take those\n",
    "        if detector.hand_fits_screen(h_k):\n",
    "          try:\n",
    "            gestures[k][1] += 1\n",
    "            results.append([gestures[k][1] , lm_norm])\n",
    "          except KeyError:\n",
    "            img = warning(img)     \n",
    "\n",
    "        else: \n",
    "          img = warning(img)        \n",
    "\n",
    "    # Main window visuals\n",
    "    img = detector.transform_draw(img)\n",
    "    img = detector.transform_connect_lines(img)\n",
    "  \n",
    "  text = [f\"[{chr(x[0])}]-{x[1][0]}: {x[1][1]}: \" for x in gestures.items()]\n",
    "  put_text(img, text)\n",
    "  cv2.imshow(\"Image\", img)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [r[0] for r in results]\n",
    "array = [r[1] for r in results]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(array)\n",
    "df = df.round(4)\n",
    "cols = df.columns\n",
    "df['target'] = target\n",
    "\n",
    "# Target should be at the beginning\n",
    "cols = cols.insert(0, 'target')\n",
    "\n",
    "df[cols].to_csv(\"out/lala.csv\", index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "34",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/federico/codes/tf/01_data_cam.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/federico/codes/tf/01_data_cam.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m {\u001b[39m1\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m34\u001b[39m], \u001b[39m2\u001b[39m:[ \u001b[39m\"\u001b[39m\u001b[39mlolo\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m0\u001b[39m], \u001b[39m3\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mlala\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3\u001b[39m]}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/federico/codes/tf/01_data_cam.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m a[\u001b[39m34\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 34"
     ]
    }
   ],
   "source": [
    "a = {1: [\"bar\",34], 2:[ \"lolo\",0], 3: [\"lala\", 3]}\n",
    "\n",
    "\n",
    "a[34]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\n",
      "lolo\n",
      "lala\n"
     ]
    }
   ],
   "source": [
    "for n in a.items():\n",
    "    print (n[1][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de9ea968598f77146e9bcf7403aafc4990d0b8fd820669c1cc9c3a0b1879543"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
